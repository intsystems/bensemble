{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bensemble Benchmark on Boston Housing\n",
    "\n",
    "This notebook assembles a lightweight benchmark for Bayesian neural network baselines on the classic Boston Housing regression task.\n",
    "\n",
    "We compare four models:\n",
    "\n",
    "- Deterministic MLP baseline trained with mean squared error.\n",
    "- Monte Carlo Dropout network.\n",
    "- Hamiltonian Monte Carlo.\n",
    "- Probabilistic Backpropagation.\n",
    "\n",
    "Each model reports RMSE and predictive negative log-likelihood on a held-out test split to illustrate both accuracy and calibrated uncertainty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160e716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu with seed 7\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED = 7\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"Running on {DEVICE} with seed {SEED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8629e404",
   "metadata": {},
   "source": [
    "## Init dataset and define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc80fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\jommm\\AppData\\Local\\Temp\\ipykernel_14356\\2644451155.py:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  df = pd.read_csv(DATA_PATH, sep='\\s+', header=None, names=COLUMN_NAMES)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = pathlib.Path('data/housing.data')\n",
    "COLUMN_NAMES = [\n",
    "    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS',\n",
    "    'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, sep='\\s+', header=None, names=COLUMN_NAMES)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e045f27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 404, Test size: 102\n"
     ]
    }
   ],
   "source": [
    "TARGET_COL = 'MEDV'\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "X = df.drop(columns=[TARGET_COL]).values.astype(np.float32)\n",
    "y = df[TARGET_COL].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=SEED\n",
    ")\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = x_scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_test_scaled = x_scaler.transform(X_test).astype(np.float32)\n",
    "y_train_scaled = y_scaler.fit_transform(y_train).astype(np.float32)\n",
    "y_test_scaled = y_scaler.transform(y_test).astype(np.float32)\n",
    "\n",
    "train_tensor_x = torch.from_numpy(X_train_scaled)\n",
    "train_tensor_y = torch.from_numpy(y_train_scaled)\n",
    "test_tensor_x = torch.from_numpy(X_test_scaled)\n",
    "test_tensor_y = torch.from_numpy(y_test_scaled)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "epochs = 1000\n",
    "train_dataset = TensorDataset(train_tensor_x, train_tensor_y)\n",
    "test_dataset = TensorDataset(test_tensor_x, test_tensor_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "full_train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "train_size = len(train_dataset)\n",
    "y_test_true = y_test.reshape(-1)\n",
    "y_train_true = y_train.reshape(-1)\n",
    "\n",
    "Y_SCALE = float(y_scaler.scale_[0])\n",
    "Y_MEAN = float(y_scaler.mean_[0])\n",
    "\n",
    "print(f\"Train size: {train_size}, Test size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce6adff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_targets(y_scaled):\n",
    "    # Return targets in the original scale as a NumPy array.\n",
    "    y_np = y_scaled.detach().cpu().numpy().reshape(-1, 1)\n",
    "    return y_scaler.inverse_transform(y_np).reshape(-1)\n",
    "\n",
    "\n",
    "def summarize_predictions(pred_samples, y_true, noise_variance=None, jitter=1e-6):\n",
    "    # Aggregate predictive samples into RMSE and negative log-likelihood.\n",
    "    pred_samples = np.array(pred_samples)\n",
    "    if pred_samples.ndim == 1:\n",
    "        pred_samples = pred_samples[None, :]\n",
    "    mean_pred = pred_samples.mean(axis=0)\n",
    "    epistemic_var = pred_samples.var(axis=0)\n",
    "    total_var = epistemic_var + jitter\n",
    "    if noise_variance is not None:\n",
    "        total_var = total_var + noise_variance\n",
    "    rmse = float(np.sqrt(np.mean((mean_pred - y_true) ** 2)))\n",
    "    nll = 0.5 * np.mean(np.log(2 * np.pi * total_var) + ((y_true - mean_pred) ** 2) / total_var)\n",
    "    return {\"rmse\": rmse, \"nll\": float(nll)}\n",
    "\n",
    "\n",
    "def print_metrics(name, metrics):\n",
    "    print(f\"{name:>20s} | RMSE: {metrics['rmse']:.4f}, NLL: {metrics['nll']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c1ef15",
   "metadata": {},
   "source": [
    "## Deterministic MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f99511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 | Train MSE: 0.0410\n",
      "Epoch 200 | Train MSE: 0.0178\n",
      "Epoch 300 | Train MSE: 0.0105\n",
      "Epoch 400 | Train MSE: 0.0077\n",
      "Epoch 500 | Train MSE: 0.0073\n",
      "Epoch 600 | Train MSE: 0.0051\n",
      "Epoch 700 | Train MSE: 0.0038\n",
      "Epoch 800 | Train MSE: 0.0038\n",
      "Epoch 900 | Train MSE: 0.0024\n",
      "Epoch 1000 | Train MSE: 0.0031\n",
      "   Deterministic MLP | RMSE: 4.0317, NLL: 34.1395\n"
     ]
    }
   ],
   "source": [
    "class DeterministicMLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dims=(64, 64)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = in_features\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def train_deterministic(model, data_loader, epochs=1000, lr=1e-3, weight_decay=1e-4):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            preds = model(xb)\n",
    "            loss = loss_fn(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            avg_loss = epoch_loss / train_size\n",
    "            print(f\"Epoch {epoch + 1:03d} | Train MSE: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_deterministic(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_preds_scaled = []\n",
    "        for xb, _ in full_train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            preds = model(xb)\n",
    "            train_preds_scaled.append(preds.cpu())\n",
    "        train_preds_scaled = torch.cat(train_preds_scaled, dim=0)\n",
    "        train_preds = inverse_transform_targets(train_preds_scaled)\n",
    "        residuals = y_train_true - train_preds\n",
    "        residual_variance = float(np.var(residuals, ddof=1))\n",
    "\n",
    "        test_preds_scaled = []\n",
    "        for xb, _ in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            preds = model(xb)\n",
    "            test_preds_scaled.append(preds.cpu())\n",
    "        test_preds_scaled = torch.cat(test_preds_scaled, dim=0)\n",
    "        test_preds = inverse_transform_targets(test_preds_scaled)\n",
    "\n",
    "    predictive_samples = np.expand_dims(test_preds, axis=0)\n",
    "    metrics = summarize_predictions(predictive_samples, y_test_true, noise_variance=residual_variance)\n",
    "    metrics['noise_variance'] = residual_variance\n",
    "    return metrics\n",
    "\n",
    "\n",
    "base_model = DeterministicMLP(in_features=X_train.shape[1]).to(DEVICE)\n",
    "train_deterministic(base_model, train_loader, epochs=epochs)\n",
    "mlp_metrics = evaluate_deterministic(base_model)\n",
    "print_metrics('Deterministic MLP', mlp_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194234be",
   "metadata": {},
   "source": [
    "## MCDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b5475b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 | Train MSE: 0.0680\n",
      "Epoch 200 | Train MSE: 0.0523\n",
      "Epoch 300 | Train MSE: 0.0387\n",
      "Epoch 400 | Train MSE: 0.0340\n",
      "Epoch 500 | Train MSE: 0.0328\n",
      "Epoch 600 | Train MSE: 0.0316\n",
      "Epoch 700 | Train MSE: 0.0288\n",
      "Epoch 800 | Train MSE: 0.0230\n",
      "Epoch 900 | Train MSE: 0.0251\n",
      "Epoch 1000 | Train MSE: 0.0216\n",
      "          MC Dropout | RMSE: 3.7852, NLL: 3.5514\n"
     ]
    }
   ],
   "source": [
    "class MCDropoutMLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dims=(64, 64), dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = in_features\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout_p))\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def train_mc_dropout(model, data_loader, epochs=400, lr=1e-3, weight_decay=1e-4):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in data_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            preds = model(xb)\n",
    "            loss = loss_fn(preds, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            avg_loss = epoch_loss / train_size\n",
    "            print(f\"Epoch {epoch + 1:03d} | Train MSE: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "def mc_dropout_predictions(model, x_tensor, n_samples=200):\n",
    "    model.train()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            outputs = model(x_tensor.to(DEVICE))\n",
    "            outputs = inverse_transform_targets(outputs.cpu())\n",
    "            preds.append(outputs)\n",
    "    return np.stack(preds, axis=0)\n",
    "\n",
    "\n",
    "def evaluate_mc_dropout(model, noise_variance):\n",
    "    test_batch = next(iter(test_loader))[0]\n",
    "    predictive_samples = mc_dropout_predictions(model, test_batch, n_samples=200)\n",
    "    metrics = summarize_predictions(predictive_samples, y_test_true, noise_variance=noise_variance)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "mc_model = MCDropoutMLP(in_features=X_train.shape[1], dropout_p=0.1).to(DEVICE)\n",
    "train_mc_dropout(mc_model, train_loader, epochs=epochs)\n",
    "\n",
    "noise_variance_base = mlp_metrics['noise_variance']\n",
    "mc_metrics = evaluate_mc_dropout(mc_model, noise_variance=noise_variance_base)\n",
    "print_metrics('MC Dropout', mc_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb037dd",
   "metadata": {},
   "source": [
    "## Hamiltonian Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc7ffc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMC accept rate: 0.937\n",
      "          HMC Linear | RMSE: 5.8315, NLL: 3.2159\n"
     ]
    }
   ],
   "source": [
    "# Hamiltonian Monte Carlo baseline (Bayesian linear regression)\n",
    "x_train_hmc = train_tensor_x.cpu()\n",
    "y_train_hmc = train_tensor_y.squeeze(1).cpu()\n",
    "x_test_hmc = test_tensor_x.cpu()\n",
    "\n",
    "n_features = x_train_hmc.shape[1]\n",
    "prior_std = 1.0\n",
    "noise_prior_std = 0.5\n",
    "\n",
    "def split_params(theta):\n",
    "    weight = theta[:n_features]\n",
    "    bias = theta[n_features]\n",
    "    log_noise = theta[n_features + 1]\n",
    "    return weight, bias, log_noise\n",
    "\n",
    "def log_posterior(theta):\n",
    "    weight, bias, log_noise = split_params(theta)\n",
    "    noise_var = torch.exp(log_noise)\n",
    "    log_prior_w = -0.5 * torch.sum((weight / prior_std) ** 2)\n",
    "    log_prior_b = -0.5 * (bias / prior_std) ** 2\n",
    "    log_prior_log_noise = -0.5 * (log_noise / noise_prior_std) ** 2\n",
    "\n",
    "    preds = x_train_hmc @ weight + bias\n",
    "    residuals = y_train_hmc - preds\n",
    "    log_likelihood = -0.5 * (\n",
    "        x_train_hmc.shape[0] * (math.log(2 * math.pi) + log_noise)\n",
    "        + torch.sum(residuals ** 2) / torch.exp(log_noise)\n",
    "    )\n",
    "    return log_likelihood + log_prior_w + log_prior_b + log_prior_log_noise\n",
    "\n",
    "def potential_and_grad(theta):\n",
    "    theta = theta.detach().requires_grad_(True)\n",
    "    potential = -log_posterior(theta)\n",
    "    grad, = torch.autograd.grad(potential, theta)\n",
    "    return potential.detach(), grad.detach()\n",
    "\n",
    "def run_hmc(initial_theta, num_samples=400, burn_in=200, step_size=0.01, leapfrog_steps=30):\n",
    "    theta = initial_theta.clone()\n",
    "    samples = []\n",
    "    accepts = 0\n",
    "    total_iters = num_samples + burn_in\n",
    "    current_U, current_grad = potential_and_grad(theta)\n",
    "    for i in range(total_iters):\n",
    "        momentum = torch.randn_like(theta)\n",
    "        current_theta = theta.clone()\n",
    "        current_K = 0.5 * torch.dot(momentum, momentum)\n",
    "\n",
    "        theta_prop = theta.clone()\n",
    "        momentum_prop = momentum.clone()\n",
    "\n",
    "        momentum_prop = momentum_prop - 0.5 * step_size * current_grad\n",
    "        theta_prop = theta_prop + step_size * momentum_prop\n",
    "\n",
    "        for _ in range(leapfrog_steps - 1):\n",
    "            U_prop, grad_prop = potential_and_grad(theta_prop)\n",
    "            momentum_prop = momentum_prop - step_size * grad_prop\n",
    "            theta_prop = theta_prop + step_size * momentum_prop\n",
    "\n",
    "        U_prop, grad_prop = potential_and_grad(theta_prop)\n",
    "        momentum_prop = momentum_prop - 0.5 * step_size * grad_prop\n",
    "\n",
    "        proposal_K = 0.5 * torch.dot(momentum_prop, momentum_prop)\n",
    "        acceptance_log_prob = (current_U + current_K) - (U_prop + proposal_K)\n",
    "        acceptance_log_prob_value = float(acceptance_log_prob)\n",
    "        if acceptance_log_prob_value >= 0 or math.log(np.random.rand()) < acceptance_log_prob_value:\n",
    "            theta = theta_prop.detach()\n",
    "            current_U = U_prop\n",
    "            current_grad = grad_prop\n",
    "            accepts += 1\n",
    "        else:\n",
    "            theta = current_theta\n",
    "            current_U, current_grad = potential_and_grad(theta)\n",
    "\n",
    "        if i >= burn_in:\n",
    "            samples.append(theta.detach().cpu().numpy())\n",
    "\n",
    "    accept_rate = accepts / total_iters\n",
    "    return np.stack(samples), accept_rate\n",
    "\n",
    "initial_theta = torch.zeros(n_features + 2)\n",
    "hmc_samples, hmc_accept = run_hmc(initial_theta, num_samples=epochs)\n",
    "print(f\"HMC accept rate: {hmc_accept:.3f}\")\n",
    "\n",
    "hmc_preds = []\n",
    "noise_vars = []\n",
    "for sample in hmc_samples:\n",
    "    weight = torch.from_numpy(sample[:n_features]).float()\n",
    "    bias = float(sample[n_features])\n",
    "    log_noise = float(sample[n_features + 1])\n",
    "    preds_scaled = x_test_hmc @ weight + bias\n",
    "    preds = inverse_transform_targets(preds_scaled.unsqueeze(1))\n",
    "    hmc_preds.append(preds)\n",
    "    noise_vars.append((Y_SCALE ** 2) * math.exp(log_noise))\n",
    "\n",
    "hmc_preds = np.array(hmc_preds)\n",
    "hmc_noise = float(np.mean(noise_vars))\n",
    "hmc_metrics = summarize_predictions(hmc_preds, y_test_true, noise_variance=hmc_noise)\n",
    "print_metrics('HMC Linear', hmc_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b033c30",
   "metadata": {},
   "source": [
    "## Probabilistic Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e5d331b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | RMSE: 6.3725 | NLL: 3.2123\n",
      "Epoch 15 | RMSE: 5.4200 | NLL: 3.3409\n",
      "Epoch 30 | RMSE: 5.3818 | NLL: 3.3993\n",
      "Epoch 45 | RMSE: 5.3808 | NLL: 3.4260\n",
      "Epoch 60 | RMSE: 5.3943 | NLL: 3.4439\n",
      "Epoch 75 | RMSE: 5.4090 | NLL: 3.4558\n",
      "Epoch 90 | RMSE: 5.4312 | NLL: 3.4685\n",
      "Probabilistic Backprop | RMSE: 5.4381, NLL: 3.4718\n"
     ]
    }
   ],
   "source": [
    "def phi(x) -> torch.Tensor:\n",
    "    return torch.exp(-0.5 * x * x) / math.sqrt(2.0 * math.pi)\n",
    "\n",
    "def Phi(x) -> torch.Tensor:\n",
    "    return 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "def relu_moments(m, v, eps: float = 1e-9):\n",
    "    v = torch.clamp(v, min=eps)\n",
    "    sigma = torch.sqrt(v)\n",
    "    alpha = torch.clamp(m / sigma, min=-10.0, max=10.0)\n",
    "    pdf = phi(alpha)\n",
    "    cdf = Phi(alpha)\n",
    "    mean = sigma * pdf + m * cdf\n",
    "    second_moment = (v + m * m) * cdf + m * sigma * pdf\n",
    "    var = torch.clamp(second_moment - mean * mean, min=eps)\n",
    "    return mean, var\n",
    "\n",
    "class ProbLinear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        d = self.in_features + 1\n",
    "        h = self.out_features\n",
    "        scale = 1.0 / math.sqrt(d)\n",
    "        self.m = nn.Parameter(scale * torch.randn(h, d, dtype=torch.float64, device=DEVICE))\n",
    "        self.v = nn.Parameter(0.5 * torch.ones(h, d, dtype=torch.float64, device=DEVICE))\n",
    "\n",
    "class PBPNet(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([ProbLinear(layer_sizes[i], layer_sizes[i + 1]) for i in range(len(layer_sizes) - 1)])\n",
    "\n",
    "    def forward_moments(self, x):\n",
    "        batch = x.shape[0]\n",
    "        mz = torch.cat([x, torch.ones(batch, 1, dtype=torch.float64, device=DEVICE)], dim=1)\n",
    "        vz = torch.zeros_like(mz)\n",
    "        for li, layer in enumerate(self.layers):\n",
    "            d = layer.in_features + 1\n",
    "            scale = 1.0 / math.sqrt(d)\n",
    "            ma = (mz @ layer.m.t()) * scale\n",
    "            term1 = (vz @ (layer.m ** 2).t())\n",
    "            term2 = ((mz ** 2) @ layer.v.t())\n",
    "            term3 = (vz @ layer.v.t())\n",
    "            va = (term1 + term2 + term3) * (scale ** 2)\n",
    "            is_last = li == len(self.layers) - 1\n",
    "            if not is_last:\n",
    "                mb, vb = relu_moments(ma, va)\n",
    "                mz = torch.cat([mb, torch.ones(batch, 1, dtype=torch.float64, device=DEVICE)], dim=1)\n",
    "                vz = torch.cat([vb, torch.zeros(batch, 1, dtype=torch.float64, device=DEVICE)], dim=1)\n",
    "            else:\n",
    "                mz = ma\n",
    "                vz = va\n",
    "        return mz, vz\n",
    "\n",
    "def logZ_gaussian_likelihood(y, mz, vz, alpha_g, beta_g, eps = 1e-9):\n",
    "    alpha_g = torch.clamp(alpha_g, min=1.0 + 1e-6)\n",
    "    sigma2_eff = torch.clamp(beta_g / (alpha_g - 1.0) + vz, min=eps)\n",
    "    return (-0.5 * ((y - mz) ** 2) / sigma2_eff - 0.5 * torch.log(2.0 * math.pi * sigma2_eff)).squeeze(-1)\n",
    "\n",
    "def gamma_adf_update_from_Z(logZ, logZ1, logZ2, alpha_old, beta_old, clamp_eps = 1e-9) :\n",
    "    r1 = torch.exp(logZ1 - logZ)\n",
    "    r2 = torch.exp(logZ2 - logZ)\n",
    "    term_alpha = (r2 / (r1 * r1)) * ((alpha_old + 1.0) / alpha_old)\n",
    "    denom_alpha = torch.clamp(term_alpha - 1.0, min=1e-12)\n",
    "    alpha_new = torch.clamp(1.0 / denom_alpha, min=1.0 + 1e-6, max=1e6)\n",
    "    termA = r2 / r1 * ((alpha_old + 1.0) / beta_old)\n",
    "    termB = r1 * (alpha_old / beta_old)\n",
    "    denom_beta = torch.clamp(termA - termB, min=clamp_eps)\n",
    "    beta_new = torch.clamp(1.0 / denom_beta, min=clamp_eps, max=1e9)\n",
    "    return alpha_new.detach(), beta_new.detach()\n",
    "\n",
    "def single_datapoint_adf_step(net, x, y, alpha_g, beta_g, step_clip = 1.5):\n",
    "    mz, vz = net.forward_moments(x.unsqueeze(0))\n",
    "    logZ = logZ_gaussian_likelihood(y.unsqueeze(0).unsqueeze(0), mz, vz, alpha_g, beta_g).mean()\n",
    "    logZ1 = logZ_gaussian_likelihood(y.unsqueeze(0).unsqueeze(0), mz, vz, alpha_g + 1.0, beta_g).mean()\n",
    "    logZ2 = logZ_gaussian_likelihood(y.unsqueeze(0).unsqueeze(0), mz, vz, alpha_g + 2.0, beta_g).mean()\n",
    "    for p in net.parameters():\n",
    "        if p.grad is not None:\n",
    "            p.grad.zero_()\n",
    "    logZ.backward()\n",
    "    for layer in net.layers:\n",
    "        gm = layer.m.grad\n",
    "        gv = layer.v.grad\n",
    "        if step_clip is not None:\n",
    "            gm = torch.clamp(gm, min=-step_clip, max=step_clip)\n",
    "            gv = torch.clamp(gv, min=-step_clip, max=step_clip)\n",
    "        m = layer.m\n",
    "        v = torch.clamp(layer.v, min=1e-10, max=1e3)\n",
    "        m_new = m + v * gm\n",
    "        v_new = v - (v * v) * (gm * gm - 2.0 * gv)\n",
    "        v_new = torch.clamp(v_new, min=1e-10, max=1e3)\n",
    "        with torch.no_grad():\n",
    "            layer.m.copy_(m_new.detach())\n",
    "            layer.v.copy_(v_new.detach())\n",
    "        layer.m.grad = None\n",
    "        layer.v.grad = None\n",
    "    alpha_g, beta_g = gamma_adf_update_from_Z(logZ.detach(), logZ1.detach(), logZ2.detach(), alpha_g, beta_g)\n",
    "    return alpha_g, beta_g\n",
    "\n",
    "def prior_refresh_epoch(net, alpha_l, beta_l, n_refresh = 1):\n",
    "    alpha_l = torch.clamp(alpha_l, min=1.0 + 1e-6)\n",
    "    for _ in range(n_refresh):\n",
    "        s2 = beta_l / (alpha_l - 1.0)\n",
    "        s2_1 = beta_l / alpha_l\n",
    "        s2_2 = beta_l / (alpha_l + 1.0)\n",
    "        Z_acc = 0.0\n",
    "        Z1_acc = 0.0\n",
    "        Z2_acc = 0.0\n",
    "        n_tot = 0\n",
    "        for layer in net.layers:\n",
    "            m = layer.m\n",
    "            v = torch.clamp(layer.v, min=1e-12)\n",
    "            m_tmp = m.detach().clone().requires_grad_(True)\n",
    "            v_tmp = v.detach().clone().requires_grad_(True)\n",
    "            sigma2 = s2 + v_tmp\n",
    "            lp = -0.5 * (m_tmp ** 2) / sigma2 - 0.5 * torch.log(2.0 * math.pi * sigma2)\n",
    "            logZ_prior = lp.sum()\n",
    "            if m_tmp.grad is not None:\n",
    "                m_tmp.grad.zero_()\n",
    "            if v_tmp.grad is not None:\n",
    "                v_tmp.grad.zero_()\n",
    "            logZ_prior.backward()\n",
    "            gm = m_tmp.grad\n",
    "            gv = v_tmp.grad\n",
    "            m_new = m + v * gm\n",
    "            v_new = v - (v * v) * (gm * gm - 2.0 * gv)\n",
    "            v_new = torch.clamp(v_new, min=1e-12, max=1e3)\n",
    "            with torch.no_grad():\n",
    "                layer.m.copy_(m_new.detach())\n",
    "                layer.v.copy_(v_new.detach())\n",
    "            def sum_logZ_given_s2(s2_local) -> torch.Tensor:\n",
    "                sigma2_local = s2_local + v\n",
    "                lp_local = -0.5 * (m ** 2) / sigma2_local - 0.5 * torch.log(2.0 * math.pi * sigma2_local)\n",
    "                return lp_local.sum()\n",
    "            Z_acc += sum_logZ_given_s2(s2).detach()\n",
    "            Z1_acc += sum_logZ_given_s2(s2_1).detach()\n",
    "            Z2_acc += sum_logZ_given_s2(s2_2).detach()\n",
    "            n_tot += m.numel()\n",
    "        logZ_avg = Z_acc / max(n_tot, 1)\n",
    "        logZ1_avg = Z1_acc / max(n_tot, 1)\n",
    "        logZ2_avg = Z2_acc / max(n_tot, 1)\n",
    "        alpha_l, beta_l = gamma_adf_update_from_Z(logZ_avg, logZ1_avg, logZ2_avg, alpha_l, beta_l)\n",
    "    return alpha_l.detach(), beta_l.detach()\n",
    "\n",
    "train_x = train_tensor_x.to(DEVICE, dtype=torch.float64)\n",
    "train_y = train_tensor_y.squeeze(1).to(DEVICE, dtype=torch.float64)\n",
    "test_x = test_tensor_x.to(DEVICE, dtype=torch.float64)\n",
    "test_y = test_tensor_y.squeeze(1).to(DEVICE, dtype=torch.float64)\n",
    "\n",
    "torch.manual_seed(SEED + 1)\n",
    "pbp_net = PBPNet([train_x.shape[1], 50, 1]).to(DEVICE)\n",
    "alpha_g = torch.tensor(6.0, dtype=torch.float64, device=DEVICE)\n",
    "beta_g = torch.tensor(6.0, dtype=torch.float64, device=DEVICE)\n",
    "alpha_l = torch.tensor(6.0, dtype=torch.float64, device=DEVICE)\n",
    "beta_l = torch.tensor(6.0, dtype=torch.float64, device=DEVICE)\n",
    "\n",
    "pbp_epochs = 100\n",
    "for epoch in range(1, pbp_epochs + 1):\n",
    "    order = torch.randperm(train_x.shape[0], device=DEVICE)\n",
    "    for idx in order:\n",
    "        x_i = train_x[idx]\n",
    "        y_i = train_y[idx]\n",
    "        alpha_g, beta_g = single_datapoint_adf_step(pbp_net, x_i, y_i, alpha_g, beta_g, step_clip=2.0)\n",
    "    alpha_l, beta_l = prior_refresh_epoch(pbp_net, alpha_l, beta_l, n_refresh=1)\n",
    "    if epoch % 15 == 0 or epoch == 1:\n",
    "        with torch.no_grad():\n",
    "            mz, vz = pbp_net.forward_moments(test_x)\n",
    "            pred_mean_scaled = mz.squeeze(-1).cpu()\n",
    "            pred_mean = inverse_transform_targets(pred_mean_scaled)\n",
    "            noise_var_scaled = torch.clamp(beta_g / (alpha_g - 1.0), min=1e-8).cpu().item()\n",
    "            total_var_scaled = noise_var_scaled + vz.squeeze(-1).cpu().numpy()\n",
    "            total_var = np.maximum(total_var_scaled * (Y_SCALE ** 2), 1e-9)\n",
    "            rmse = float(np.sqrt(np.mean((pred_mean - y_test_true) ** 2)))\n",
    "            nll = 0.5 * np.log(2 * math.pi * total_var) + 0.5 * ((y_test_true - pred_mean) ** 2) / total_var\n",
    "            print(f\"Epoch {epoch:02d} | RMSE: {rmse:.4f} | NLL: {float(np.mean(nll)):.4f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_mz, test_vz = pbp_net.forward_moments(test_x)\n",
    "    pred_mean_scaled = test_mz.squeeze(-1)\n",
    "    pred_mean = inverse_transform_targets(pred_mean_scaled.cpu())\n",
    "    epistemic_var_scaled = test_vz.squeeze(-1).cpu().numpy()\n",
    "    epistemic_var = np.maximum(epistemic_var_scaled * (Y_SCALE ** 2), 1e-9)\n",
    "    noise_var_scaled = torch.clamp(beta_g / (alpha_g - 1.0), min=1e-8).cpu().item()\n",
    "    noise_var = float(noise_var_scaled * (Y_SCALE ** 2))\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    pbp_samples = rng.normal(loc=pred_mean, scale=np.sqrt(epistemic_var), size=(300, pred_mean.shape[0]))\n",
    "    pbp_metrics = summarize_predictions(pbp_samples, y_test_true, noise_variance=noise_var)\n",
    "    print_metrics('Probabilistic Backprop', pbp_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "004f3eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>rmse</th>\n",
       "      <th>nll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deterministic MLP</td>\n",
       "      <td>4.031665</td>\n",
       "      <td>34.139454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MC Dropout</td>\n",
       "      <td>3.785248</td>\n",
       "      <td>3.551396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HMC Linear</td>\n",
       "      <td>5.831530</td>\n",
       "      <td>3.215919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Probabilistic Backprop</td>\n",
       "      <td>5.438124</td>\n",
       "      <td>3.471774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model      rmse        nll\n",
       "0       Deterministic MLP  4.031665  34.139454\n",
       "1              MC Dropout  3.785248   3.551396\n",
       "2              HMC Linear  5.831530   3.215919\n",
       "3  Probabilistic Backprop  5.438124   3.471774"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame([\n",
    "    {\"model\": \"Deterministic MLP\", \"rmse\": mlp_metrics['rmse'], \"nll\": mlp_metrics['nll']},\n",
    "    {\"model\": \"MC Dropout\", \"rmse\": mc_metrics['rmse'], \"nll\": mc_metrics['nll']},\n",
    "    {\"model\": \"HMC Linear\", \"rmse\": hmc_metrics['rmse'], \"nll\": hmc_metrics['nll']},\n",
    "    {\"model\": \"Probabilistic Backprop\", \"rmse\": pbp_metrics['rmse'], \"nll\": pbp_metrics['nll']},\n",
    "])\n",
    "results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "windows_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
