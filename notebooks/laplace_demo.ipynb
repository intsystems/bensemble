{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890645f4-12e3-42b0-902b-0e6e9d6ca79f",
   "metadata": {
    "id": "890645f4-12e3-42b0-902b-0e6e9d6ca79f"
   },
   "source": [
    "# Kronecker-Factored Laplace Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d8f1a-2d93-471e-8b3e-85cae423bff3",
   "metadata": {},
   "source": [
    "_Kronecker-factored Laplace approximation (KFLA)_ is a post hoc method for model weight posterior approximation based on the Laplace approximation of the negative log-posterior and Kronecker factoring of this approximation for tractability. In Bensemble, we implement this powerful post hoc tool in the `LaplaceApproximation` class. In this notebook, we'll take you through basic usage of this class for probabilistic machine learning tasks and model ensembling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42daa56b-71d6-48da-b3cd-79e07a288341",
   "metadata": {
    "id": "42daa56b-71d6-48da-b3cd-79e07a288341"
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95adbda-2ef2-48ee-8a72-5993fe7711ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa2d37-0637-4abd-8368-c0cd17dc1f34",
   "metadata": {
    "id": "acfa2d37-0637-4abd-8368-c0cd17dc1f34"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bensemble.methods.laplace_approximation import LaplaceApproximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3053358-a8cd-4aab-95e6-9977d9833876",
   "metadata": {
    "id": "e3053358-a8cd-4aab-95e6-9977d9833876"
   },
   "source": [
    "## Example model and training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdae860-11f9-41c8-8f41-851f2fbe9e92",
   "metadata": {},
   "source": [
    "Below we provide an example of a small neural network model and a training loop to train it on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba35b0-ea6e-4232-bca5-337fc5b6b75e",
   "metadata": {
    "id": "daba35b0-ea6e-4232-bca5-337fc5b6b75e"
   },
   "outputs": [],
   "source": [
    "# Neural Network Model\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"Simple feedforward neural network for MNIST\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_dim=784, hidden_dims=[1200, 1200], output_dim=10, dropout_rate=0.0\n",
    "    ):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685ff75-0bac-440b-a41c-3a3cddcc4143",
   "metadata": {
    "id": "2685ff75-0bac-440b-a41c-3a3cddcc4143"
   },
   "outputs": [],
   "source": [
    "\"\"\"Training Function\"\"\"\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, num_epochs=50, lr=1e-3, weight_decay=1e-4\n",
    "):\n",
    "    \"\"\"Train a neural network model\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += predicted.eq(target).sum().item()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_accuracy = 100.0 * train_correct / train_total\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        val_accuracy = 100.0 * val_correct / val_total\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracy:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce452120-6f1e-47cd-af97-97b66c966bf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce452120-6f1e-47cd-af97-97b66c966bf5",
    "outputId": "b367329f-9939-42a5-8a6f-68e3c5953130"
   },
   "outputs": [],
   "source": [
    "\"\"\"Training pipeline\"\"\"\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data preparation\n",
    "print(\"Loading and preparing data...\")\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"../data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(\n",
    "    train_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Model training\n",
    "print(\"\\nTraining model...\")\n",
    "model = SimpleNN(input_dim=784, hidden_dims=[1200, 1200], output_dim=10)\n",
    "\n",
    "training_history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=2,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6e13f-4e5d-4c67-a67c-61b9df2d92ca",
   "metadata": {
    "id": "35a6e13f-4e5d-4c67-a67c-61b9df2d92ca"
   },
   "source": [
    "## Creating Laplace approximation class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7351b5d-44ea-4ff9-b28b-4e388876ccdc",
   "metadata": {},
   "source": [
    "The first thing you would want to do with `LaplaceApproximation` is create an instance of the class and then run either the `compute_posterior` or the `fit` method to compute the Kronecker factors used to sample models for prediction later. Both `compute_posterior` and `fit` take a PyTorch DataLoader instance, prior precision and sample count as input and internally compute Kronecker factors for the approximation as well as their square roots for sampling.\n",
    "\n",
    "Since KFLA's main strength is its post hoc computation, the standard usage scenario for `LaplaceApproximation` is creation of the approximation for a pretrained model. Thus, the `pretrained` parameter of `LaplaceApproximation` is set to `True` by default, and in this case the `fit` method does exactly the same thing as the `compute_posterior` one. In case you really need it, you also have the option to set `pretrained=True` when creating the Laplace approximation instance. In this case the `fit` method will train the model with a simple built-in training loop before computing the posterior approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fce12b-719c-4a2c-8d7e-f2beca125902",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37fce12b-719c-4a2c-8d7e-f2beca125902",
    "outputId": "ee9dffcd-6c99-48d6-f6cd-7b5441dcc47f"
   },
   "outputs": [],
   "source": [
    "print(\"Creating KFLA...\")\n",
    "\n",
    "# Make sure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "### Create the Laplace approximation. ###\n",
    "# Option to set likelihood to 'regression' for regression tasks with an MSE loss\n",
    "# Verbose is off by default, but here we turn it on for demonstration purposes.\n",
    "# You can toggle verbose on and off with the .toggle_verbose() method\n",
    "laplace = LaplaceApproximation(model, likelihood=\"classification\", verbose=True)\n",
    "\n",
    "# Compute the posterior\n",
    "print(\"Computing posterior...\")\n",
    "laplace.compute_posterior(\n",
    "    train_loader=train_loader, prior_precision=1.0, num_samples=len(train_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd5204-1179-46ab-b8ae-f6c849f26a34",
   "metadata": {
    "id": "cadd5204-1179-46ab-b8ae-f6c849f26a34"
   },
   "source": [
    "## KFLA evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eaded8-52d1-4a7e-8509-7c568d470d86",
   "metadata": {},
   "source": [
    "In the `predict` method, `LaplaceApproximation` samples `n_samples` models from the posterior using precomputed sampling factors and makes a prediction with each one, after which the results are aggregated to produce a final answer as well as the uncertainty of the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f76fb-da35-40ab-b436-6373ec05ebc0",
   "metadata": {
    "id": "345f76fb-da35-40ab-b436-6373ec05ebc0"
   },
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "\n",
    "\n",
    "def evaluate_laplace_model(laplace, test_loader, num_samples=10, temperature=1.0):\n",
    "    \"\"\"Evaluate the model with Laplace approximation\"\"\"\n",
    "    laplace.model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    uncertainties = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            mean_probs, uncertainty = laplace.predict(\n",
    "                data, n_samples=num_samples, temperature=temperature\n",
    "            )\n",
    "\n",
    "            _, predicted = mean_probs.max(1)\n",
    "\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            uncertainties.extend(uncertainty.cpu().numpy())\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    avg_uncertainty = np.mean(uncertainties)\n",
    "\n",
    "    return accuracy, avg_uncertainty, uncertainties\n",
    "\n",
    "\n",
    "def evaluate_standard_model(model, test_loader):\n",
    "    \"\"\"Evaluate the standard model\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8850d6b-e61b-4f20-b37b-f7369f8555cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8850d6b-e61b-4f20-b37b-f7369f8555cf",
    "outputId": "844f3c54-b996-497d-92b0-cb4a6744555e"
   },
   "outputs": [],
   "source": [
    "# Test standard model first\n",
    "print(\"\\nEvaluating standard model...\")\n",
    "standard_accuracy = evaluate_standard_model(model, test_loader)\n",
    "print(f\"Standard Model Accuracy: {standard_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86215ff7-64a2-4dea-9597-768ad177531f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86215ff7-64a2-4dea-9597-768ad177531f",
    "outputId": "a9420dba-b29b-4e0e-cc5d-6a2cc2fa35b4"
   },
   "outputs": [],
   "source": [
    "accuracy, uncertainty, _ = evaluate_laplace_model(\n",
    "    laplace, test_loader, num_samples=10, temperature=1.0\n",
    ")\n",
    "print(\n",
    "    f\"10 samples, T = 1.0: Accuracy = {accuracy:.2f}%, Uncertainty = {uncertainty:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b893beb-8bb9-4a7e-ae7c-a8267f17f385",
   "metadata": {},
   "source": [
    "While we have optimized model sampling as much as could be optimized, model inference is certainly not the fastest due to random sampling and additional matrix multiplication for each sampled model. A way to make predictions faster is to sample an ensemble of models beforehand and then make predictions with the ensemble afterwards. We'll show an example of model sampling later in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad0209-729b-492c-ad0b-74a3294da13a",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7ad0209-729b-492c-ad0b-74a3294da13a",
    "outputId": "8f89f5e2-1e7b-425c-a0d8-e2dc92130586"
   },
   "outputs": [],
   "source": [
    "print(\"\\nTesting Laplace with different sample counts:\")\n",
    "for n_samples in [10, 20, 50, 100]:\n",
    "    accuracy, uncertainty, _ = evaluate_laplace_model(\n",
    "        laplace, test_loader, num_samples=n_samples, temperature=1.0\n",
    "    )\n",
    "    print(\n",
    "        f\"{n_samples} samples, T = 1.0: Accuracy = {accuracy:.2f}%, Uncertainty = {uncertainty:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2be46-d6de-4d9c-b8fd-299e8067e9a2",
   "metadata": {},
   "source": [
    "You can also change the posterior temperature in the `predict` method to control the model's certainty about its predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9196d03-49fe-465d-bfb3-698edf6fa150",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9196d03-49fe-465d-bfb3-698edf6fa150",
    "outputId": "4b7d1df6-7fb5-4697-aa9f-8a8722a439eb"
   },
   "outputs": [],
   "source": [
    "print(\"\\nTesting Laplace with different temperatures:\")\n",
    "temperatures = [0.1, 0.5, 1, 2]\n",
    "\n",
    "for temp in temperatures:\n",
    "    accuracy, uncertainty, _ = evaluate_laplace_model(\n",
    "        laplace, test_loader, num_samples=10, temperature=temp\n",
    "    )\n",
    "    print(\n",
    "        f\"Temperature {temp}: Accuracy = {accuracy:.2f}%, Uncertainty = {uncertainty:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de360c4-b0cb-4ad4-9c10-2a781b2a15eb",
   "metadata": {},
   "source": [
    "Here, we analyze average uncertainty for correct and incorrect predictions. Note that uncertainty is on average slightly larger for incorrect answers, which is expected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6423ae8a-267b-4168-b370-e9ff6d4cbc5e",
   "metadata": {
    "id": "6423ae8a-267b-4168-b370-e9ff6d4cbc5e"
   },
   "outputs": [],
   "source": [
    "def analyze_uncertainty_by_accuracy(laplace, test_loader, num_samples=10):\n",
    "    \"\"\"Analyze uncertainty for correct vs incorrect predictions\"\"\"\n",
    "    laplace.model.eval()\n",
    "    correct_uncertainties = []\n",
    "    incorrect_uncertainties = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            mean_probs, uncertainty = laplace.predict(data, n_samples=num_samples)\n",
    "\n",
    "            _, predicted = mean_probs.max(1)\n",
    "\n",
    "            # Separate uncertainties for correct and incorrect predictions\n",
    "            correct_mask = predicted.eq(target)\n",
    "            incorrect_mask = ~correct_mask\n",
    "\n",
    "            if correct_mask.any():\n",
    "                correct_uncertainties.extend(uncertainty[correct_mask].cpu().numpy())\n",
    "            if incorrect_mask.any():\n",
    "                incorrect_uncertainties.extend(\n",
    "                    uncertainty[incorrect_mask].cpu().numpy()\n",
    "                )\n",
    "\n",
    "    return correct_uncertainties, incorrect_uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6ef8b-9451-4105-af52-0e1a4112ccc0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19e6ef8b-9451-4105-af52-0e1a4112ccc0",
    "outputId": "9ebba80e-3fa8-4d8f-9243-3e344513f197"
   },
   "outputs": [],
   "source": [
    "correct_unc, incorrect_unc = analyze_uncertainty_by_accuracy(laplace, test_loader)\n",
    "\n",
    "print(\"\\nUncertainty Analysis:\")\n",
    "print(f\"Average uncertainty for correct predictions: {np.mean(correct_unc):.4f}\")\n",
    "print(f\"Average uncertainty for incorrect predictions: {np.mean(incorrect_unc):.4f}\")\n",
    "print(f\"Number of correct predictions: {len(correct_unc)}\")\n",
    "print(f\"Number of incorrect predictions: {len(incorrect_unc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f8f7b-c565-47b7-8275-2473ecdb14c7",
   "metadata": {},
   "source": [
    "Let's also take a look at single predictions and their uncertainties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cd6656-ea0e-4932-b14b-f76e303f3290",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67cd6656-ea0e-4932-b14b-f76e303f3290",
    "outputId": "9e1b7400-ac69-48dd-a1ce-d3fee24025e7"
   },
   "outputs": [],
   "source": [
    "# Example of getting predictions with uncertainty for a single batch\n",
    "print(\"\\nTesting on a single batch...\")\n",
    "data_iter = iter(test_loader)\n",
    "test_data, test_target = next(data_iter)\n",
    "test_data, test_target = test_data.to(device), test_target.to(device)\n",
    "\n",
    "mean_probs, uncertainty = laplace.predict(test_data, n_samples=10)\n",
    "_, predicted = mean_probs.max(1)\n",
    "\n",
    "print(\"Predictions for first 10 test samples:\")\n",
    "for i in range(min(10, len(test_data))):\n",
    "    print(\n",
    "        f\"  Sample {i+1}: True={test_target[i].item()}, Pred={predicted[i].item()}, \"\n",
    "        f\"Uncertainty={uncertainty[i].item():.4f}, \"\n",
    "        f\"Correct={predicted[i].eq(test_target[i]).item()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc0ce8-24ea-4bc7-a922-5350cbc599b5",
   "metadata": {},
   "source": [
    "## Model sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce551ff-663f-41da-af43-8f98bc3dcdfe",
   "metadata": {},
   "source": [
    "The sampling pipeline is pretty straight-forward: just call the `sample_models` method with your desired `n_models` amount of models to sample. The method returns a Python list of `nn.Module` instances which then can be used for sampling. Keep in mind that for ensembles of large models you'll likely need a decent amount of memory to store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gSGfs4sK2Fth",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gSGfs4sK2Fth",
    "outputId": "beb123f3-c14f-43e4-80e3-4485ee895237"
   },
   "outputs": [],
   "source": [
    "model_samples = laplace.sample_models(n_models=10)\n",
    "model_samples[0]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "e3053358-a8cd-4aab-95e6-9977d9833876"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
