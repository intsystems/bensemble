{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Bensemble Pro tip: visit our project's GitHub page . About Bensemble Bensemble is a lightweight library based on PyTorch that implements several Bayesian (probabilistic) ensembling methods for linear neural network models. Bayesian ensembling is a method of making predictions in machine learning that incorporate uncertainty about the predictions into the estimate. This is done by approximating the posterior distribution over the weights of the neural network and then using the distribution to either sample predictions or ensembles of models, the predictions of which are then averaged and analyzed for uncertainty estimation. Navigation To get a quickstart on library installation and basic usage, go to the Getting Started page. To get a more detailed insight into how each method works, check out the User Guide section. There you can find guides for each implemented method as well as a description of the general interface of our ensemble classes (go to Interface for more). For specific details on the API, go to the API section. We also provide links to the papers describing all the implemented methods in the References section.","title":"Home"},{"location":"#welcome-to-bensemble","text":"Pro tip: visit our project's GitHub page .","title":"Welcome to Bensemble"},{"location":"#about-bensemble","text":"Bensemble is a lightweight library based on PyTorch that implements several Bayesian (probabilistic) ensembling methods for linear neural network models. Bayesian ensembling is a method of making predictions in machine learning that incorporate uncertainty about the predictions into the estimate. This is done by approximating the posterior distribution over the weights of the neural network and then using the distribution to either sample predictions or ensembles of models, the predictions of which are then averaged and analyzed for uncertainty estimation.","title":"About Bensemble"},{"location":"#navigation","text":"To get a quickstart on library installation and basic usage, go to the Getting Started page. To get a more detailed insight into how each method works, check out the User Guide section. There you can find guides for each implemented method as well as a description of the general interface of our ensemble classes (go to Interface for more). For specific details on the API, go to the API section. We also provide links to the papers describing all the implemented methods in the References section.","title":"Navigation"},{"location":"getting-started/","text":"Getting Started To get started with Bensemble, all you'll need is Python with at least version 3.10. Installation pip install bensemble Basic usage All Bensemble classes are built as wrappers around PyTorch models that are subclasses of the torch.nn.Module class. All you need then is a model, a torch.utils.data.DataLoader for training and you're good to go! import torch from torch import nn from torch.utils.data import DataLoader from bensemble.methods.variational_inference import VariationalEnsemble # Create a model model = nn.Sequential( nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1) ) # Choose your data train_data = ... test_data = ... # Create a DataLoader instance train_loader = DataLoader(data) # Create ensemble ensemble = VariationalEnsemble(model) # Train model and its posterior ensemble.fit(train_loader) # Make predictions for x in test_data: print(ensemble.predict(x, n_samples=10))","title":"Getting started"},{"location":"getting-started/#getting-started","text":"To get started with Bensemble, all you'll need is Python with at least version 3.10.","title":"Getting Started"},{"location":"getting-started/#installation","text":"pip install bensemble","title":"Installation"},{"location":"getting-started/#basic-usage","text":"All Bensemble classes are built as wrappers around PyTorch models that are subclasses of the torch.nn.Module class. All you need then is a model, a torch.utils.data.DataLoader for training and you're good to go! import torch from torch import nn from torch.utils.data import DataLoader from bensemble.methods.variational_inference import VariationalEnsemble # Create a model model = nn.Sequential( nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1) ) # Choose your data train_data = ... test_data = ... # Create a DataLoader instance train_loader = DataLoader(data) # Create ensemble ensemble = VariationalEnsemble(model) # Train model and its posterior ensemble.fit(train_loader) # Make predictions for x in test_data: print(ensemble.predict(x, n_samples=10))","title":"Basic usage"},{"location":"references/","text":"References Here you can find links to the papers describing the algorithms implemented in Bensemble. We of course cannot leave the authors these algorithms uncredited. Practical variational inference - Alex Graves \"Practical Variational Inference for Neural Networks\" (2011) Kronecker-factored Laplace approximation - Hippolyt Ritter, Aleksandar Botev, David Barber \"A Scalable Laplace Approximation for Neural Networks\" (2018) Renyi divergence variational inference - Yingzhen Li, Richard E. Turner \"R\u00e9nyi Divergence Variational Inference\" (2016) Probabilistic backpropagation - Jose Miguel Hernandez-Lobato, Ryan Adams \"Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks\" (2015)","title":"References"},{"location":"references/#references","text":"Here you can find links to the papers describing the algorithms implemented in Bensemble. We of course cannot leave the authors these algorithms uncredited. Practical variational inference - Alex Graves \"Practical Variational Inference for Neural Networks\" (2011) Kronecker-factored Laplace approximation - Hippolyt Ritter, Aleksandar Botev, David Barber \"A Scalable Laplace Approximation for Neural Networks\" (2018) Renyi divergence variational inference - Yingzhen Li, Richard E. Turner \"R\u00e9nyi Divergence Variational Inference\" (2016) Probabilistic backpropagation - Jose Miguel Hernandez-Lobato, Ryan Adams \"Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks\" (2015)","title":"References"},{"location":"api/additional-classes/","text":"Additional Classes BayesianLinear class bensemble.methods.variational_inference.BayesianLinear Bayesian linear layer with mean-field variational inference. Constructor BayesianLinear(in_features, out_features, prior_sigma=1.0) Parameters: in_features (int): Size of each input sample out_features (int): Size of each output sample prior_sigma (float, optional): Standard deviation of the Gaussian prior. Default: 1.0 Attributes: in_features (int): Input feature dimension out_features (int): Output feature dimension prior_sigma (float): Prior standard deviation sampling (bool): Sampling mode flag (True for training/MC, False for mean prediction) w_mu (torch.Tensor): Mean of weight variational distribution w_rho (torch.Tensor): Raw parameter for weight standard deviation b_mu (torch.Tensor): Mean of bias variational distribution b_rho (torch.Tensor): Raw parameter for bias standard deviation forward Performs forward pass through the Bayesian linear layer. Parameters: x (torch.Tensor): Input tensor of shape (batch_size, in_features) Returns: torch.Tensor : Output tensor of shape (batch_size, out_features) Behavior: In sampling mode ( sampling=True ): Uses local reparameterization trick In mean mode ( sampling=False ): Uses only the mean parameters kl_divergence Computes KL divergence between variational posterior and prior. Returns: torch.Tensor : KL divergence scalar value Formula: KL(q(w|\u03b8) || p(w)) where p(w) = N(0, prior_sigma\u00b2) and q(w|\u03b8) = N(\u03bc, \u03c3\u00b2) GaussianLikelihood class bensemble.methods.variational_inference.GaussianLikelihood Gaussian likelihood module for regression tasks with learnable noise parameter. Constructor GaussianLikelihood(init_log_sigma=-2.0) Parameters: init_log_sigma (float, optional): Initial value for log(sigma). Default: -2.0 Attributes: log_sigma (torch.nn.Parameter): Learnable log standard deviation parameter forward Computes negative log-likelihood for Gaussian distribution. Parameters: preds (torch.Tensor): Model predictions target (torch.Tensor): Ground truth targets Returns: torch.Tensor : Negative log-likelihood scalar value Formula: -0.5 * (target - preds)\u00b2 / \u03c3\u00b2 + log(\u03c3) get_noise_sigma Gets the current noise standard deviation value. Returns: - float : Noise sigma value (detached from computation graph) PBPNet class bensemble.methods.probabilistic_backpropagation.PBPNet A neural network architecture based on ProbLinear layers with analytical moment propagation for Probabilistic Backpropagation (PBP). This network implements moment propagation through Gaussian approximations, enabling efficient Bayesian inference without sampling during forward passes. It uses analytical formulas to propagate means and variances through linear transformations and ReLU activations. Constructor def __init__( self, layer_sizes: List[int], dtype: torch.dtype = torch.float64, device: Optional[torch.device] = None, ) Initialize a PBPNet with specified architecture. Parameters: layer_sizes (List[int]): List specifying the number of neurons in each layer. Example: [input_dim, hidden_dim1, hidden_dim2, ..., output_dim] Minimum length: 2 (input and output layers) dtype (torch.dtype): Data type for network parameters and computations. Default: torch.float64 device (Optional[torch.device]): Device for network parameters and computations (CPU/GPU). Default: CPU device Raises: ValueError : If layer_sizes has less than 2 elements. Example: # Create a network with 10 inputs, 2 hidden layers (50 and 20 neurons), and 1 output model = PBPNet(layer_sizes=[10, 50, 20, 1], dtype=torch.float64, device=torch.device(\"cuda\")) forward_moments def forward_moments(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.torch.Tensor] Propagate input through the network while tracking means and variances analytically. This method implements the core moment propagation algorithm for PBP: Appends a bias term (constant 1) to the input with zero variance For each layer: Computes mean and variance of linear transformation For non-final layers: applies ReLU activation with moment matching For final layer: returns linear transformation moments Parameters: x (torch.Tensor): Input tensor of shape (batch_size, input_dim) . Must have 2 dimensions (batch dimension required) Batch size must be \u2265 1 Returns: Tuple[torch.Tensor, torch.Tensor] : Tuple containing: mean : Output mean tensor of shape (batch_size, output_dim) variance : Output variance tensor of shape (batch_size, output_dim) Raises: AssertionError : If input tensor does not have 2 dimensions or batch size is 0. Mathematical Details: For a linear layer with input moments (m_z, v_z) and layer parameters (m, v) : - Output mean: m_a = (m_z @ m^T) / sqrt(d) where d = input_dim + 1 - Output variance: v_a = (v_z @ m^2 + m_z^2 @ v + v_z @ v) / d For ReLU activation (non-final layers): - Uses relu_moments() function to compute post-activation moments - Bias term is added back after activation Example: # Create network model = PBPNet(layer_sizes=[5, 10, 1]) # Input batch of 3 samples, 5 features each x = torch.randn(3, 5, dtype=torch.float64) # Get predictive moments mean, variance = model.forward_moments(x) # mean.shape: (3, 1) # variance.shape: (3, 1) Usage Notes Precision : Defaults to torch.float64 for numerical stability in moment computations. Device Management : Automatically moves input tensors to the network's device and dtype. Batch Processing : Efficiently processes batches using matrix operations. No Training Interface : This class only provides forward moment propagation; training is handled by ProbabilisticBackpropagation .","title":"Additional Classes"},{"location":"api/additional-classes/#additional-classes","text":"","title":"Additional Classes"},{"location":"api/additional-classes/#bayesianlinear","text":"class bensemble.methods.variational_inference.BayesianLinear Bayesian linear layer with mean-field variational inference.","title":"BayesianLinear"},{"location":"api/additional-classes/#constructor","text":"BayesianLinear(in_features, out_features, prior_sigma=1.0) Parameters: in_features (int): Size of each input sample out_features (int): Size of each output sample prior_sigma (float, optional): Standard deviation of the Gaussian prior. Default: 1.0 Attributes: in_features (int): Input feature dimension out_features (int): Output feature dimension prior_sigma (float): Prior standard deviation sampling (bool): Sampling mode flag (True for training/MC, False for mean prediction) w_mu (torch.Tensor): Mean of weight variational distribution w_rho (torch.Tensor): Raw parameter for weight standard deviation b_mu (torch.Tensor): Mean of bias variational distribution b_rho (torch.Tensor): Raw parameter for bias standard deviation","title":"Constructor"},{"location":"api/additional-classes/#forward","text":"Performs forward pass through the Bayesian linear layer. Parameters: x (torch.Tensor): Input tensor of shape (batch_size, in_features) Returns: torch.Tensor : Output tensor of shape (batch_size, out_features) Behavior: In sampling mode ( sampling=True ): Uses local reparameterization trick In mean mode ( sampling=False ): Uses only the mean parameters","title":"forward"},{"location":"api/additional-classes/#kl_divergence","text":"Computes KL divergence between variational posterior and prior. Returns: torch.Tensor : KL divergence scalar value Formula: KL(q(w|\u03b8) || p(w)) where p(w) = N(0, prior_sigma\u00b2) and q(w|\u03b8) = N(\u03bc, \u03c3\u00b2)","title":"kl_divergence"},{"location":"api/additional-classes/#gaussianlikelihood","text":"class bensemble.methods.variational_inference.GaussianLikelihood Gaussian likelihood module for regression tasks with learnable noise parameter.","title":"GaussianLikelihood"},{"location":"api/additional-classes/#constructor_1","text":"GaussianLikelihood(init_log_sigma=-2.0) Parameters: init_log_sigma (float, optional): Initial value for log(sigma). Default: -2.0 Attributes: log_sigma (torch.nn.Parameter): Learnable log standard deviation parameter","title":"Constructor"},{"location":"api/additional-classes/#forward_1","text":"Computes negative log-likelihood for Gaussian distribution. Parameters: preds (torch.Tensor): Model predictions target (torch.Tensor): Ground truth targets Returns: torch.Tensor : Negative log-likelihood scalar value Formula: -0.5 * (target - preds)\u00b2 / \u03c3\u00b2 + log(\u03c3)","title":"forward"},{"location":"api/additional-classes/#get_noise_sigma","text":"Gets the current noise standard deviation value. Returns: - float : Noise sigma value (detached from computation graph)","title":"get_noise_sigma"},{"location":"api/additional-classes/#pbpnet","text":"class bensemble.methods.probabilistic_backpropagation.PBPNet A neural network architecture based on ProbLinear layers with analytical moment propagation for Probabilistic Backpropagation (PBP). This network implements moment propagation through Gaussian approximations, enabling efficient Bayesian inference without sampling during forward passes. It uses analytical formulas to propagate means and variances through linear transformations and ReLU activations.","title":"PBPNet"},{"location":"api/additional-classes/#constructor_2","text":"def __init__( self, layer_sizes: List[int], dtype: torch.dtype = torch.float64, device: Optional[torch.device] = None, ) Initialize a PBPNet with specified architecture. Parameters: layer_sizes (List[int]): List specifying the number of neurons in each layer. Example: [input_dim, hidden_dim1, hidden_dim2, ..., output_dim] Minimum length: 2 (input and output layers) dtype (torch.dtype): Data type for network parameters and computations. Default: torch.float64 device (Optional[torch.device]): Device for network parameters and computations (CPU/GPU). Default: CPU device Raises: ValueError : If layer_sizes has less than 2 elements. Example: # Create a network with 10 inputs, 2 hidden layers (50 and 20 neurons), and 1 output model = PBPNet(layer_sizes=[10, 50, 20, 1], dtype=torch.float64, device=torch.device(\"cuda\"))","title":"Constructor"},{"location":"api/additional-classes/#forward_moments","text":"def forward_moments(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.torch.Tensor] Propagate input through the network while tracking means and variances analytically. This method implements the core moment propagation algorithm for PBP: Appends a bias term (constant 1) to the input with zero variance For each layer: Computes mean and variance of linear transformation For non-final layers: applies ReLU activation with moment matching For final layer: returns linear transformation moments Parameters: x (torch.Tensor): Input tensor of shape (batch_size, input_dim) . Must have 2 dimensions (batch dimension required) Batch size must be \u2265 1 Returns: Tuple[torch.Tensor, torch.Tensor] : Tuple containing: mean : Output mean tensor of shape (batch_size, output_dim) variance : Output variance tensor of shape (batch_size, output_dim) Raises: AssertionError : If input tensor does not have 2 dimensions or batch size is 0. Mathematical Details: For a linear layer with input moments (m_z, v_z) and layer parameters (m, v) : - Output mean: m_a = (m_z @ m^T) / sqrt(d) where d = input_dim + 1 - Output variance: v_a = (v_z @ m^2 + m_z^2 @ v + v_z @ v) / d For ReLU activation (non-final layers): - Uses relu_moments() function to compute post-activation moments - Bias term is added back after activation Example: # Create network model = PBPNet(layer_sizes=[5, 10, 1]) # Input batch of 3 samples, 5 features each x = torch.randn(3, 5, dtype=torch.float64) # Get predictive moments mean, variance = model.forward_moments(x) # mean.shape: (3, 1) # variance.shape: (3, 1)","title":"forward_moments"},{"location":"api/additional-classes/#usage-notes","text":"Precision : Defaults to torch.float64 for numerical stability in moment computations. Device Management : Automatically moves input tensors to the network's device and dtype. Batch Processing : Efficiently processes batches using matrix operations. No Training Interface : This class only provides forward moment propagation; training is handled by ProbabilisticBackpropagation .","title":"Usage Notes"},{"location":"api/core/","text":"Core BaseBayesianEnsemble class bensemble.core.base.BaseBayesianEnsemble Base class for all Bayesian ensemble methods. Constructor def __init__(self, model: nn.Module, **kwargs): Parameters model ( nn.Module ): Base PyTorch model architecture to use for ensemble members kwargs : Additional implementation-specific parameters Attributes model ( nn.Module ): Reference to the base model architecture is_fitted ( bool ): Flag indicating whether the ensemble has been trained ensemble ( list ): Container for ensemble members (implementation-specific) fit @abc.abstractmethod def fit( self, train_loader: torch.utils.data.DataLoader, val_loader: Optional[torch.utils.data.DataLoader] = None, **kwargs, ) -> Dict[str, List[float]]: Trains the ensemble using the provided data loaders. Parameters train_loader ( DataLoader ): DataLoader for training data val_loader ( DataLoader , optional): DataLoader for validation data kwargs : Additional training parameters (implementation-specific) Returns Dict[str, List[float]] : Training history/metrics (implementation-specific format) Notes Must be implemented by all subclasses Should update self.is_fitted to True upon successful training predict @abc.abstractmethod def predict( self, X: torch.Tensor, n_samples: int = 100 ) -> Tuple[torch.Tensor, torch.Tensor]: Makes predictions with uncertainty estimates. Parameters X ( torch.Tensor ): Input tensor of shape (batch_size, ...) n_samples ( int , default=100): Number of forward passes/samples for uncertainty estimation Returns Tuple[torch.Tensor, torch.Tensor] : First tensor: Predictions (typically mean) Second tensor: Uncertainty estimates (e.g., variance, standard deviation) Notes Should raise an error if self.is_fitted is False The exact form of uncertainty estimates depends on the implementation sample_models @abc.abstractmethod def sample_models(self, n_models: int = 10) -> List[nn.Module]: Samples individual models from the posterior distribution. Parameters n_models ( int , default=10): Number of models to sample Returns List[nn.Module] : List of sampled PyTorch models Notes Intended for online generation and maintenance of ensemble members Sampled models should be ready for inference _get_ensemble_state @abc.abstractmethod def _get_ensemble_state(self) -> Dict[str, Any]: Gets the internal state of the ensemble for serialization. Returns Dict[str, Any] : Dictionary containing all necessary state information Notes Used internally by save() method Implementation should include all parameters needed to restore the ensemble _set_ensemble_state @abc.abstractmethod def _set_ensemble_state(self, state: Dict[str, Any]): Restores the internal state of the ensemble from serialized data. Parameters state ( Dict[str, Any] ): Dictionary containing saved state information Notes Used internally by load() method Should handle version compatibility if needed save def save(self, path: str): Saves the trained ensemble to disk. Parameters path ( str ): File path where the ensemble will be saved Saves Base model state dictionary Ensemble state (via _get_ensemble_state() ) is_fitted flag File Format PyTorch checkpoint file ( .pt or .pth ) load def load(self, path: str): Loads a trained ensemble from disk. Parameters path ( str ): File path to the saved ensemble Loads Base model state dictionary Ensemble state (via _set_ensemble_state() ) is_fitted flag Raises FileNotFoundError : If the specified path doesn't exist Runtime errors if the saved format is incompatible Implementation Notes Subclassing : All abstract methods must be implemented by subclasses Device Management : Implementations should handle device placement (CPU/GPU) State Management : Ensure _get_ensemble_state() and _set_ensemble_state() are comprehensive Error Handling : Check is_fitted flag in predict() and sample_models() Serialization : Consider versioning for saved models to handle future changes","title":"Core"},{"location":"api/core/#core","text":"","title":"Core"},{"location":"api/core/#basebayesianensemble","text":"class bensemble.core.base.BaseBayesianEnsemble Base class for all Bayesian ensemble methods.","title":"BaseBayesianEnsemble"},{"location":"api/core/#constructor","text":"def __init__(self, model: nn.Module, **kwargs): Parameters model ( nn.Module ): Base PyTorch model architecture to use for ensemble members kwargs : Additional implementation-specific parameters Attributes model ( nn.Module ): Reference to the base model architecture is_fitted ( bool ): Flag indicating whether the ensemble has been trained ensemble ( list ): Container for ensemble members (implementation-specific)","title":"Constructor"},{"location":"api/core/#fit","text":"@abc.abstractmethod def fit( self, train_loader: torch.utils.data.DataLoader, val_loader: Optional[torch.utils.data.DataLoader] = None, **kwargs, ) -> Dict[str, List[float]]: Trains the ensemble using the provided data loaders. Parameters train_loader ( DataLoader ): DataLoader for training data val_loader ( DataLoader , optional): DataLoader for validation data kwargs : Additional training parameters (implementation-specific) Returns Dict[str, List[float]] : Training history/metrics (implementation-specific format) Notes Must be implemented by all subclasses Should update self.is_fitted to True upon successful training","title":"fit"},{"location":"api/core/#predict","text":"@abc.abstractmethod def predict( self, X: torch.Tensor, n_samples: int = 100 ) -> Tuple[torch.Tensor, torch.Tensor]: Makes predictions with uncertainty estimates. Parameters X ( torch.Tensor ): Input tensor of shape (batch_size, ...) n_samples ( int , default=100): Number of forward passes/samples for uncertainty estimation Returns Tuple[torch.Tensor, torch.Tensor] : First tensor: Predictions (typically mean) Second tensor: Uncertainty estimates (e.g., variance, standard deviation) Notes Should raise an error if self.is_fitted is False The exact form of uncertainty estimates depends on the implementation","title":"predict"},{"location":"api/core/#sample_models","text":"@abc.abstractmethod def sample_models(self, n_models: int = 10) -> List[nn.Module]: Samples individual models from the posterior distribution. Parameters n_models ( int , default=10): Number of models to sample Returns List[nn.Module] : List of sampled PyTorch models Notes Intended for online generation and maintenance of ensemble members Sampled models should be ready for inference","title":"sample_models"},{"location":"api/core/#_get_ensemble_state","text":"@abc.abstractmethod def _get_ensemble_state(self) -> Dict[str, Any]: Gets the internal state of the ensemble for serialization. Returns Dict[str, Any] : Dictionary containing all necessary state information Notes Used internally by save() method Implementation should include all parameters needed to restore the ensemble","title":"_get_ensemble_state"},{"location":"api/core/#_set_ensemble_state","text":"@abc.abstractmethod def _set_ensemble_state(self, state: Dict[str, Any]): Restores the internal state of the ensemble from serialized data. Parameters state ( Dict[str, Any] ): Dictionary containing saved state information Notes Used internally by load() method Should handle version compatibility if needed","title":"_set_ensemble_state"},{"location":"api/core/#save","text":"def save(self, path: str): Saves the trained ensemble to disk. Parameters path ( str ): File path where the ensemble will be saved Saves Base model state dictionary Ensemble state (via _get_ensemble_state() ) is_fitted flag File Format PyTorch checkpoint file ( .pt or .pth )","title":"save"},{"location":"api/core/#load","text":"def load(self, path: str): Loads a trained ensemble from disk. Parameters path ( str ): File path to the saved ensemble Loads Base model state dictionary Ensemble state (via _set_ensemble_state() ) is_fitted flag Raises FileNotFoundError : If the specified path doesn't exist Runtime errors if the saved format is incompatible","title":"load"},{"location":"api/core/#implementation-notes","text":"Subclassing : All abstract methods must be implemented by subclasses Device Management : Implementations should handle device placement (CPU/GPU) State Management : Ensure _get_ensemble_state() and _set_ensemble_state() are comprehensive Error Handling : Check is_fitted flag in predict() and sample_models() Serialization : Consider versioning for saved models to handle future changes","title":"Implementation Notes"},{"location":"api/methods/","text":"Methods VariationalEnsemble class bensemble.methods.variational_inference.VariationalEnsemble Main ensemble class for Bayesian neural networks with variational inference. Constructor VariationalEnsemble( model: nn.Module, likelihood: Optional[nn.Module] = None, learning_rate: float = 1e-3, prior_sigma: float = 1.0, auto_convert: bool = True, **kwargs ) Parameters: model (nn.Module): Base neural network model likelihood (nn.Module, optional): Likelihood module. If None, uses GaussianLikelihood() . Default: None learning_rate (float, optional): Learning rate for optimizer. Default: 1e-3 prior_sigma (float, optional): Prior standard deviation for Bayesian layers. Default: 1.0 auto_convert (bool, optional): Automatically convert Linear layers to BayesianLinear. Default: True **kwargs : Additional arguments passed to parent class Attributes: prior_sigma (float): Prior standard deviation learning_rate (float): Learning rate likelihood (nn.Module): Likelihood module optimizer (torch.optim.Optimizer): Optimizer instance (initialized during fit ) model (nn.Module): Bayesian model is_fitted (bool): Training status flag fit Trains the Bayesian ensemble model. Parameters: train_loader (torch.utils.data.DataLoader): Training data loader val_loader (torch.utils.data.DataLoader, optional): Validation data loader. Default: None epochs (int, optional): Number of training epochs. Default: 100 kl_weight (float, optional): Weight for KL divergence term. Default: 0.1 verbose (bool, optional): Print training progress. Default: True **kwargs : Additional arguments including: device (str/torch.device): Device to use for training Returns: dict : Training history with keys: train_loss , nll , kl Raises: ValueError : If data loaders are improperly configured predict Makes predictions with uncertainty estimates using Monte Carlo sampling. Parameters: X (torch.Tensor): Input tensor of shape (batch_size, features) n_samples (int, optional): Number of Monte Carlo samples. Default: 100 Returns: Tuple[torch.Tensor, torch.Tensor] : Mean and standard deviation predictions Raises: RuntimeError : If model is not fitted Note: Predictions are moved to CPU and detached from computation graph. sample_models Samples deterministic models from the variational posterior. Parameters: n_models (int, optional): Number of models to sample. Default: 10 Returns: List[nn.Module] : List of sampled neural network models Raises: RuntimeError : If model is not fitted Note: Each sampled model has fixed weights (sampling disabled) and is in evaluation mode. Notes Device Management: The ensemble automatically moves models to the appropriate device during training and prediction. Sampling Behavior: Training: Sampling enabled with local reparameterization Prediction: Sampling enabled for MC estimates Sampled models: Sampling permanently disabled (deterministic) KL Weighting: The kl_weight parameter controls the trade-off between data fit and model complexity. Noise Estimation: When using GaussianLikelihood , the noise sigma is learned during training and used for prediction uncertainty. Memory: sample_models creates deep copies of the model, which may be memory intensive for large models. LaplaceApproximation class bensemble.methods.laplace_approximation.LaplaceApproximation Constructor def __init__( self, model: nn.Module, pretrained: bool = True, likelihood: str = \"classification\", verbose: bool = False, ) Initializes the Laplace approximation ensemble. Parameters: model (nn.Module): The neural network model to apply Laplace approximation to. pretrained (bool, optional): Whether the model is already trained. If False , the model will be trained during fit() . Default: True . likelihood (str, optional): The type of likelihood function. Must be either \"classification\" or \"regression\" . Default: \"classification\" . verbose (bool, optional): Whether to print progress information during training and posterior computation. Default: False . Raises: ValueError : If likelihood is not \"classification\" or \"regression\" . Example: model = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 2)) la = LaplaceApproximation(model, likelihood=\"classification\", verbose=True) toggle_verbose def toggle_verbose(self) -> None Toggles verbose mode on or off. Example: la.toggle_verbose() # Turns verbose on if off, off if on fit def fit( self, train_loader: torch.utils.data.DataLoader, val_loader: Optional[torch.utils.data.DataLoader] = None, num_epochs: int = 100, lr: float = 1e-3, prior_precision: float = 1.0, num_samples: int = 1000, ) -> Dict[str, List[float]] Trains the model (if not pretrained) and computes the Laplace approximation posterior. Parameters: train_loader (DataLoader): DataLoader for training data. val_loader (DataLoader, optional): DataLoader for validation data. Currently unused but included for API consistency. num_epochs (int, optional): Number of training epochs if model is not pretrained. Default: 100 . lr (float, optional): Learning rate for training if model is not pretrained. Default: 1e-3 . prior_precision (float, optional): Precision (inverse variance) of the Gaussian prior. Default: 1.0 . num_samples (int, optional): Number of samples to use for estimating Kronecker factors. Default: 1000 . Returns: Dict[str, List[float]] : Training history containing loss values. Example: history = la.fit( train_loader=train_loader, num_epochs=50, lr=0.001, prior_precision=0.1, num_samples=500 ) compute_posterior def compute_posterior( self, train_loader: DataLoader, prior_precision: float = 0.0, num_samples: int = 1000, ) -> None Computes the Kronecker-factored Laplace approximation posterior. This method estimates the posterior distribution by computing Kronecker-factored curvature matrices for each linear layer in the network. Parameters: train_loader (DataLoader): DataLoader containing training data for posterior estimation. prior_precision (float, optional): Precision (inverse variance) of the Gaussian prior. Default: 0.0 . num_samples (int, optional): Number of samples to use for estimating Kronecker factors. Default: 1000 . Example: la.compute_posterior( train_loader=train_loader, prior_precision=1.0, num_samples=2000 ) sample_models def sample_models( self, n_models: int = 10, temperature: float = 1.0 ) -> List[nn.Module] Samples weight matrices from the matrix normal posterior distribution. Parameters: n_models (int, optional): Number of model samples to generate. Default: 10 . temperature (float, optional): Scaling factor for the covariance during sampling. Higher values increase diversity. Default: 1.0 . Returns: List[nn.Module] : List of sampled model instances with different weight configurations. Example: sampled_models = la.sample_models(n_models=5, temperature=0.5) for model in sampled_models: predictions = model(test_data) predict def predict( self, X: torch.Tensor, n_samples: int = 100, temperature: float = 1.0 ) -> Tuple[torch.Tensor, torch.Tensor] Computes predictive distribution using Monte Carlo sampling from the posterior. Parameters: X (torch.Tensor): Input tensor of shape (batch_size, input_dim) . n_samples (int, optional): Number of Monte Carlo samples to draw. Default: 100 . temperature (float, optional): Scaling factor for the covariance during sampling. Default: 1.0 . Returns: Tuple[torch.Tensor, torch.Tensor] : For classification: (mean_probabilities, predictive_entropy) For regression: (mean_prediction, predictive_variance) Example: # For classification mean_probs, uncertainty = la.predict(X_test, n_samples=50) # For regression mean_pred, variance = la.predict(X_test, n_samples=50) Notes Model Requirements: The method currently supports nn.Linear layers. Other layer types are ignored. Device Handling: The class automatically detects and uses the same device as the input model. State Management: After calling fit() or compute_posterior() , the internal state ( kronecker_factors , sampling_factors , etc.) is populated and ready for sampling. Memory Usage: Sampling large models with many Monte Carlo samples may require significant memory. Likelihood Types: \"classification\" : Uses cross-entropy loss with softmax output. \"regression\" : Uses mean squared error loss. VariationalRenyi class bensemble.methods.variational_renyi.VariationalRenyi Constructor def __init__( self, model: nn.Module, alpha: float = 1.0, prior_sigma: float = 1.0, initial_rho: float = -2.0, **kwargs ) Initializes the VariationalRenyi model. Parameters: model (nn.Module): A deterministic PyTorch neural network model. Only Linear layers will be made Bayesian. alpha (float, optional): R\u00e9nyi alpha parameter. When \u03b1=1, this reduces to standard Variational Inference. Default: 1.0. prior_sigma (float, optional): Standard deviation of the Gaussian prior distribution for weights. Default: 1.0. initial_rho (float, optional): Initial value for the rho parameter used in variational weight parameterization. Default: -2.0. **kwargs : Additional arguments passed to the parent BaseBayesianEnsemble class. fit def fit( self, train_loader: torch.utils.data.DataLoader, val_loader: Optional[torch.utils.data.DataLoader] = None, num_epochs: int = 100, lr: float = 5e-4, n_samples: int = 10, grad_clip: Optional[float] = 5.0, **kwargs ) -> Dict[str, List[float]] Trains the model using the Variational R\u00e9nyi bound. Parameters: train_loader (DataLoader): DataLoader for training data. val_loader (DataLoader, optional): DataLoader for validation data. If provided, validation loss is computed each epoch. Default: None. num_epochs (int, optional): Number of training epochs. Default: 100. lr (float, optional): Learning rate for the Adam optimizer. Default: 5e-4. n_samples (int, optional): Number of Monte Carlo samples used to approximate the loss each iteration. Default: 10. grad_clip (float, optional): Maximum absolute value for gradient clipping. If None, no clipping is applied. Default: 5.0. **kwargs : Additional keyword arguments (not currently used). Returns: Dict[str, List[float]] : Training history containing: \"train_loss\" : List of training losses for each epoch. \"val_loss\" : List of validation losses for each epoch (if val_loader provided). predict def predict( self, X: torch.Tensor, n_samples: int = 100 ) -> Tuple[torch.Tensor, torch.Tensor] Makes predictions using the trained model with uncertainty quantification. Parameters: X (torch.Tensor): Input tensor of shape (batch_size, ...). n_samples (int, optional): Number of stochastic forward passes to sample. Default: 100. Returns: Tuple[torch.Tensor, torch.Tensor] : A tuple containing: Mean prediction across all samples. All sampled predictions of shape (n_samples, batch_size, ...). Raises: RuntimeError : If the model hasn't been fitted (call fit() first). sample_models def sample_models(self, n_models: int = 10) -> List[nn.Module] Samples multiple deterministic models from the learned variational distribution. Parameters: n_models (int, optional): Number of models to sample. Default: 10. Returns: List[nn.Module] : List of sampled PyTorch models with frozen weights. forward def forward(self, x: torch.Tensor) -> torch.Tensor Performs a single stochastic forward pass through the network. Parameters: x (torch.Tensor): Input tensor. Returns: torch.Tensor : Model output. For regression: shape (batch_size, 1); for classification: shape (batch_size, num_classes). Notes The model converts all nn.Linear layers in the provided template to Bayesian layers with learnable mean (mu) and variance (rho) parameters. For regression tasks, the likelihood uses a Gaussian distribution with fixed sigma=0.1. For classification tasks, the likelihood uses cross-entropy loss. When alpha=1.0 , the method reduces to standard Variational Inference (ELBO). The is_fitted attribute is set to True after successful training. The class inherits from BaseBayesianEnsemble and may have additional methods from the parent class. ProbabilisticBackpropagation class bensemble.methods.probabilistic_backpropagation.ProbabilisticBackpropagation Constructor def __init__( self, model: Optional[nn.Module] = None, layer_sizes: Optional[List[int]] = None, noise_alpha: float = 6.0, noise_beta: float = 6.0, weight_alpha: float = 6.0, weight_beta: float = 6.0, dtype: torch.dtype = torch.float64, device: Optional[torch.device] = None, ) Parameters: model (Optional[nn.Module]): Pre-initialized PBP model. If None, layer_sizes must be provided. layer_sizes (Optional[List[int]]): List specifying the number of neurons in each layer (e.g., [input_dim, hidden_dim, output_dim] ). noise_alpha (float): Prior shape parameter for observation noise Gamma distribution. Default: 6.0 noise_beta (float): Prior rate parameter for observation noise Gamma distribution. Default: 6.0 weight_alpha (float): Prior shape parameter for weight precision Gamma distribution. Default: 6.0 weight_beta (float): Prior rate parameter for weight precision Gamma distribution. Default: 6.0 dtype (torch.dtype): Data type for computations. Default: torch.float64 device (Optional[torch.device]): Device for computations (CPU/GPU). Default: torch.device(\"cpu\") Raises: ValueError : If neither model nor layer_sizes are specified. fit def fit( self, train_loader: torch.utils.data.DataLoader, val_loader: Optional[torch.utils.data.DataLoader] = None, num_epochs: int = 100, step_clip: Optional[float] = 2.0, prior_refresh: int = 1, **kwargs ) -> Dict[str, List[float]] Train the PBP model using Assumed Density Filtering (ADF). Parameters: train_loader (DataLoader): DataLoader for training data. val_loader (Optional[DataLoader]): DataLoader for validation data. Default: None num_epochs (int): Number of training epochs. Default: 100 step_clip (Optional[float]): Gradient clipping value for ADF updates. If None, no clipping. Default: 2.0 prior_refresh (int): Number of prior refresh steps per epoch. Default: 1 **kwargs : Additional arguments (ignored, for compatibility). Returns: Dict[str, List[float]] : Training history containing: train_rmse : List of training RMSE values per epoch train_nlpd : List of training Negative Log Predictive Density (NLPD) values per epoch val_rmse : List of validation RMSE values per epoch (if val_loader provided) val_nlpd : List of validation NLPD values per epoch (if val_loader provided) predict def predict( self, X: torch.Tensor, n_samples: int = 100 ) -> Tuple[torch.Tensor, torch.Tensor] Generate predictive distribution for input data. Parameters: X (torch.Tensor): Input tensor of shape (n_samples, n_features) . n_samples (int): Number of Monte Carlo samples to draw from predictive distribution. Default: 100 Returns: Tuple[torch.Tensor, torch.Tensor] : Tuple containing: mean : Predictive mean tensor of shape (n_samples, output_dim) samples : Monte Carlo samples tensor of shape (n_samples, n_samples, output_dim) Raises: RuntimeError : If model has not been fitted. noise_variance def noise_variance(self) -> torch.Tensor Get the estimated observation noise variance. Returns: torch.Tensor : Scalar tensor representing the noise variance. sample_models def sample_models(self, n_models: int = 10) -> List[nn.Module] Sample deterministic neural networks from the posterior distribution. Parameters: n_models (int): Number of models to sample. Default: 10 Returns: List[nn.Module] : List of sampled models as nn.Sequential instances with ReLU activations. Notes The implementation uses double precision ( torch.float64 ) by default for numerical stability. Training is performed using Assumed Density Filtering (ADF) with online updates. The model provides both epistemic (model) and aleatoric (noise) uncertainty. The prior refresh mechanism helps maintain stable hyperparameter updates during training. The predict method returns both the predictive mean and Monte Carlo samples for full distribution analysis.","title":"Methods"},{"location":"api/methods/#methods","text":"","title":"Methods"},{"location":"api/methods/#variationalensemble","text":"class bensemble.methods.variational_inference.VariationalEnsemble Main ensemble class for Bayesian neural networks with variational inference.","title":"VariationalEnsemble"},{"location":"api/methods/#constructor","text":"VariationalEnsemble( model: nn.Module, likelihood: Optional[nn.Module] = None, learning_rate: float = 1e-3, prior_sigma: float = 1.0, auto_convert: bool = True, **kwargs ) Parameters: model (nn.Module): Base neural network model likelihood (nn.Module, optional): Likelihood module. If None, uses GaussianLikelihood() . Default: None learning_rate (float, optional): Learning rate for optimizer. Default: 1e-3 prior_sigma (float, optional): Prior standard deviation for Bayesian layers. Default: 1.0 auto_convert (bool, optional): Automatically convert Linear layers to BayesianLinear. Default: True **kwargs : Additional arguments passed to parent class Attributes: prior_sigma (float): Prior standard deviation learning_rate (float): Learning rate likelihood (nn.Module): Likelihood module optimizer (torch.optim.Optimizer): Optimizer instance (initialized during fit ) model (nn.Module): Bayesian model is_fitted (bool): Training status flag","title":"Constructor"},{"location":"api/methods/#fit","text":"Trains the Bayesian ensemble model. Parameters: train_loader (torch.utils.data.DataLoader): Training data loader val_loader (torch.utils.data.DataLoader, optional): Validation data loader. Default: None epochs (int, optional): Number of training epochs. Default: 100 kl_weight (float, optional): Weight for KL divergence term. Default: 0.1 verbose (bool, optional): Print training progress. Default: True **kwargs : Additional arguments including: device (str/torch.device): Device to use for training Returns: dict : Training history with keys: train_loss , nll , kl Raises: ValueError : If data loaders are improperly configured","title":"fit"},{"location":"api/methods/#predict","text":"Makes predictions with uncertainty estimates using Monte Carlo sampling. Parameters: X (torch.Tensor): Input tensor of shape (batch_size, features) n_samples (int, optional): Number of Monte Carlo samples. Default: 100 Returns: Tuple[torch.Tensor, torch.Tensor] : Mean and standard deviation predictions Raises: RuntimeError : If model is not fitted Note: Predictions are moved to CPU and detached from computation graph.","title":"predict"},{"location":"api/methods/#sample_models","text":"Samples deterministic models from the variational posterior. Parameters: n_models (int, optional): Number of models to sample. Default: 10 Returns: List[nn.Module] : List of sampled neural network models Raises: RuntimeError : If model is not fitted Note: Each sampled model has fixed weights (sampling disabled) and is in evaluation mode.","title":"sample_models"},{"location":"api/methods/#notes","text":"Device Management: The ensemble automatically moves models to the appropriate device during training and prediction. Sampling Behavior: Training: Sampling enabled with local reparameterization Prediction: Sampling enabled for MC estimates Sampled models: Sampling permanently disabled (deterministic) KL Weighting: The kl_weight parameter controls the trade-off between data fit and model complexity. Noise Estimation: When using GaussianLikelihood , the noise sigma is learned during training and used for prediction uncertainty. Memory: sample_models creates deep copies of the model, which may be memory intensive for large models.","title":"Notes"},{"location":"api/methods/#laplaceapproximation","text":"class bensemble.methods.laplace_approximation.LaplaceApproximation","title":"LaplaceApproximation"},{"location":"api/methods/#constructor_1","text":"def __init__( self, model: nn.Module, pretrained: bool = True, likelihood: str = \"classification\", verbose: bool = False, ) Initializes the Laplace approximation ensemble. Parameters: model (nn.Module): The neural network model to apply Laplace approximation to. pretrained (bool, optional): Whether the model is already trained. If False , the model will be trained during fit() . Default: True . likelihood (str, optional): The type of likelihood function. Must be either \"classification\" or \"regression\" . Default: \"classification\" . verbose (bool, optional): Whether to print progress information during training and posterior computation. Default: False . Raises: ValueError : If likelihood is not \"classification\" or \"regression\" . Example: model = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 2)) la = LaplaceApproximation(model, likelihood=\"classification\", verbose=True)","title":"Constructor"},{"location":"api/methods/#toggle_verbose","text":"def toggle_verbose(self) -> None Toggles verbose mode on or off. Example: la.toggle_verbose() # Turns verbose on if off, off if on","title":"toggle_verbose"},{"location":"api/methods/#fit_1","text":"def fit( self, train_loader: torch.utils.data.DataLoader, val_loader: Optional[torch.utils.data.DataLoader] = None, num_epochs: int = 100, lr: float = 1e-3, prior_precision: float = 1.0, num_samples: int = 1000, ) -> Dict[str, List[float]] Trains the model (if not pretrained) and computes the Laplace approximation posterior. Parameters: train_loader (DataLoader): DataLoader for training data. val_loader (DataLoader, optional): DataLoader for validation data. Currently unused but included for API consistency. num_epochs (int, optional): Number of training epochs if model is not pretrained. Default: 100 . lr (float, optional): Learning rate for training if model is not pretrained. Default: 1e-3 . prior_precision (float, optional): Precision (inverse variance) of the Gaussian prior. Default: 1.0 . num_samples (int, optional): Number of samples to use for estimating Kronecker factors. Default: 1000 . Returns: Dict[str, List[float]] : Training history containing loss values. Example: history = la.fit( train_loader=train_loader, num_epochs=50, lr=0.001, prior_precision=0.1, num_samples=500 )","title":"fit"},{"location":"api/methods/#compute_posterior","text":"def compute_posterior( self, train_loader: DataLoader, prior_precision: float = 0.0, num_samples: int = 1000, ) -> None Computes the Kronecker-factored Laplace approximation posterior. This method estimates the posterior distribution by computing Kronecker-factored curvature matrices for each linear layer in the network. Parameters: train_loader (DataLoader): DataLoader containing training data for posterior estimation. prior_precision (float, optional): Precision (inverse variance) of the Gaussian prior. Default: 0.0 . num_samples (int, optional): Number of samples to use for estimating Kronecker factors. Default: 1000 . Example: la.compute_posterior( train_loader=train_loader, prior_precision=1.0, num_samples=2000 )","title":"compute_posterior"},{"location":"api/methods/#sample_models_1","text":"def sample_models( self, n_models: int = 10, temperature: float = 1.0 ) -> List[nn.Module] Samples weight matrices from the matrix normal posterior distribution. Parameters: n_models (int, optional): Number of model samples to generate. Default: 10 . temperature (float, optional): Scaling factor for the covariance during sampling. Higher values increase diversity. Default: 1.0 . Returns: List[nn.Module] : List of sampled model instances with different weight configurations. Example: sampled_models = la.sample_models(n_models=5, temperature=0.5) for model in sampled_models: predictions = model(test_data)","title":"sample_models"},{"location":"api/methods/#predict_1","text":"def predict( self, X: torch.Tensor, n_samples: int = 100, temperature: float = 1.0 ) -> Tuple[torch.Tensor, torch.Tensor] Computes predictive distribution using Monte Carlo sampling from the posterior. Parameters: X (torch.Tensor): Input tensor of shape (batch_size, input_dim) . n_samples (int, optional): Number of Monte Carlo samples to draw. Default: 100 . temperature (float, optional): Scaling factor for the covariance during sampling. Default: 1.0 . Returns: Tuple[torch.Tensor, torch.Tensor] : For classification: (mean_probabilities, predictive_entropy) For regression: (mean_prediction, predictive_variance) Example: # For classification mean_probs, uncertainty = la.predict(X_test, n_samples=50) # For regression mean_pred, variance = la.predict(X_test, n_samples=50)","title":"predict"},{"location":"api/methods/#notes_1","text":"Model Requirements: The method currently supports nn.Linear layers. Other layer types are ignored. Device Handling: The class automatically detects and uses the same device as the input model. State Management: After calling fit() or compute_posterior() , the internal state ( kronecker_factors , sampling_factors , etc.) is populated and ready for sampling. Memory Usage: Sampling large models with many Monte Carlo samples may require significant memory. Likelihood Types: \"classification\" : Uses cross-entropy loss with softmax output. \"regression\" : Uses mean squared error loss.","title":"Notes"},{"location":"api/methods/#variationalrenyi","text":"class bensemble.methods.variational_renyi.VariationalRenyi","title":"VariationalRenyi"},{"location":"api/methods/#constructor_2","text":"def __init__( self, model: nn.Module, alpha: float = 1.0, prior_sigma: float = 1.0, initial_rho: float = -2.0, **kwargs ) Initializes the VariationalRenyi model. Parameters: model (nn.Module): A deterministic PyTorch neural network model. Only Linear layers will be made Bayesian. alpha (float, optional): R\u00e9nyi alpha parameter. When \u03b1=1, this reduces to standard Variational Inference. Default: 1.0. prior_sigma (float, optional): Standard deviation of the Gaussian prior distribution for weights. Default: 1.0. initial_rho (float, optional): Initial value for the rho parameter used in variational weight parameterization. Default: -2.0. **kwargs : Additional arguments passed to the parent BaseBayesianEnsemble class.","title":"Constructor"},{"location":"api/methods/#fit_2","text":"def fit( self, train_loader: torch.utils.data.DataLoader, val_loader: Optional[torch.utils.data.DataLoader] = None, num_epochs: int = 100, lr: float = 5e-4, n_samples: int = 10, grad_clip: Optional[float] = 5.0, **kwargs ) -> Dict[str, List[float]] Trains the model using the Variational R\u00e9nyi bound. Parameters: train_loader (DataLoader): DataLoader for training data. val_loader (DataLoader, optional): DataLoader for validation data. If provided, validation loss is computed each epoch. Default: None. num_epochs (int, optional): Number of training epochs. Default: 100. lr (float, optional): Learning rate for the Adam optimizer. Default: 5e-4. n_samples (int, optional): Number of Monte Carlo samples used to approximate the loss each iteration. Default: 10. grad_clip (float, optional): Maximum absolute value for gradient clipping. If None, no clipping is applied. Default: 5.0. **kwargs : Additional keyword arguments (not currently used). Returns: Dict[str, List[float]] : Training history containing: \"train_loss\" : List of training losses for each epoch. \"val_loss\" : List of validation losses for each epoch (if val_loader provided).","title":"fit"},{"location":"api/methods/#predict_2","text":"def predict( self, X: torch.Tensor, n_samples: int = 100 ) -> Tuple[torch.Tensor, torch.Tensor] Makes predictions using the trained model with uncertainty quantification. Parameters: X (torch.Tensor): Input tensor of shape (batch_size, ...). n_samples (int, optional): Number of stochastic forward passes to sample. Default: 100. Returns: Tuple[torch.Tensor, torch.Tensor] : A tuple containing: Mean prediction across all samples. All sampled predictions of shape (n_samples, batch_size, ...). Raises: RuntimeError : If the model hasn't been fitted (call fit() first).","title":"predict"},{"location":"api/methods/#sample_models_2","text":"def sample_models(self, n_models: int = 10) -> List[nn.Module] Samples multiple deterministic models from the learned variational distribution. Parameters: n_models (int, optional): Number of models to sample. Default: 10. Returns: List[nn.Module] : List of sampled PyTorch models with frozen weights.","title":"sample_models"},{"location":"api/methods/#forward","text":"def forward(self, x: torch.Tensor) -> torch.Tensor Performs a single stochastic forward pass through the network. Parameters: x (torch.Tensor): Input tensor. Returns: torch.Tensor : Model output. For regression: shape (batch_size, 1); for classification: shape (batch_size, num_classes).","title":"forward"},{"location":"api/methods/#notes_2","text":"The model converts all nn.Linear layers in the provided template to Bayesian layers with learnable mean (mu) and variance (rho) parameters. For regression tasks, the likelihood uses a Gaussian distribution with fixed sigma=0.1. For classification tasks, the likelihood uses cross-entropy loss. When alpha=1.0 , the method reduces to standard Variational Inference (ELBO). The is_fitted attribute is set to True after successful training. The class inherits from BaseBayesianEnsemble and may have additional methods from the parent class.","title":"Notes"},{"location":"api/methods/#probabilisticbackpropagation","text":"class bensemble.methods.probabilistic_backpropagation.ProbabilisticBackpropagation","title":"ProbabilisticBackpropagation"},{"location":"api/methods/#constructor_3","text":"def __init__( self, model: Optional[nn.Module] = None, layer_sizes: Optional[List[int]] = None, noise_alpha: float = 6.0, noise_beta: float = 6.0, weight_alpha: float = 6.0, weight_beta: float = 6.0, dtype: torch.dtype = torch.float64, device: Optional[torch.device] = None, ) Parameters: model (Optional[nn.Module]): Pre-initialized PBP model. If None, layer_sizes must be provided. layer_sizes (Optional[List[int]]): List specifying the number of neurons in each layer (e.g., [input_dim, hidden_dim, output_dim] ). noise_alpha (float): Prior shape parameter for observation noise Gamma distribution. Default: 6.0 noise_beta (float): Prior rate parameter for observation noise Gamma distribution. Default: 6.0 weight_alpha (float): Prior shape parameter for weight precision Gamma distribution. Default: 6.0 weight_beta (float): Prior rate parameter for weight precision Gamma distribution. Default: 6.0 dtype (torch.dtype): Data type for computations. Default: torch.float64 device (Optional[torch.device]): Device for computations (CPU/GPU). Default: torch.device(\"cpu\") Raises: ValueError : If neither model nor layer_sizes are specified.","title":"Constructor"},{"location":"api/methods/#fit_3","text":"def fit( self, train_loader: torch.utils.data.DataLoader, val_loader: Optional[torch.utils.data.DataLoader] = None, num_epochs: int = 100, step_clip: Optional[float] = 2.0, prior_refresh: int = 1, **kwargs ) -> Dict[str, List[float]] Train the PBP model using Assumed Density Filtering (ADF). Parameters: train_loader (DataLoader): DataLoader for training data. val_loader (Optional[DataLoader]): DataLoader for validation data. Default: None num_epochs (int): Number of training epochs. Default: 100 step_clip (Optional[float]): Gradient clipping value for ADF updates. If None, no clipping. Default: 2.0 prior_refresh (int): Number of prior refresh steps per epoch. Default: 1 **kwargs : Additional arguments (ignored, for compatibility). Returns: Dict[str, List[float]] : Training history containing: train_rmse : List of training RMSE values per epoch train_nlpd : List of training Negative Log Predictive Density (NLPD) values per epoch val_rmse : List of validation RMSE values per epoch (if val_loader provided) val_nlpd : List of validation NLPD values per epoch (if val_loader provided)","title":"fit"},{"location":"api/methods/#predict_3","text":"def predict( self, X: torch.Tensor, n_samples: int = 100 ) -> Tuple[torch.Tensor, torch.Tensor] Generate predictive distribution for input data. Parameters: X (torch.Tensor): Input tensor of shape (n_samples, n_features) . n_samples (int): Number of Monte Carlo samples to draw from predictive distribution. Default: 100 Returns: Tuple[torch.Tensor, torch.Tensor] : Tuple containing: mean : Predictive mean tensor of shape (n_samples, output_dim) samples : Monte Carlo samples tensor of shape (n_samples, n_samples, output_dim) Raises: RuntimeError : If model has not been fitted.","title":"predict"},{"location":"api/methods/#noise_variance","text":"def noise_variance(self) -> torch.Tensor Get the estimated observation noise variance. Returns: torch.Tensor : Scalar tensor representing the noise variance.","title":"noise_variance"},{"location":"api/methods/#sample_models_3","text":"def sample_models(self, n_models: int = 10) -> List[nn.Module] Sample deterministic neural networks from the posterior distribution. Parameters: n_models (int): Number of models to sample. Default: 10 Returns: List[nn.Module] : List of sampled models as nn.Sequential instances with ReLU activations.","title":"sample_models"},{"location":"api/methods/#notes_3","text":"The implementation uses double precision ( torch.float64 ) by default for numerical stability. Training is performed using Assumed Density Filtering (ADF) with online updates. The model provides both epistemic (model) and aleatoric (noise) uncertainty. The prior refresh mechanism helps maintain stable hyperparameter updates during training. The predict method returns both the predictive mean and Monte Carlo samples for full distribution analysis.","title":"Notes"},{"location":"user-guide/bayes-by-backprop/","text":"Probabilistic Backpropagation Pro tip: check out our probabilistic backpropagation demo . We implement the probabilistic backpropagation (or Bayes by Backprop) method described by Jose Miguel Hernandez-Lobato and Ryan Adams (2015) in the bensemble.methods.probabilistic_backpropagation.ProbabilisticBackpropagation class (phew, that's a long one). This is a method that combines the backpropagation algorithm with Bayesian machine learning for efficient posterior optimization during model training. Note: currently the algorithm is implemented for regression with a Gaussian likelihood. How it works Probabilistic Backpropagation (PBP) pushes the classical \u201cuncertainty in weights\u201d idea quite literally. PBP maintains a Gaussian distribution for model weights: w_{ij} = \\mathcal{N}(\\mu_{ij}, \\sigma^2_{ij}). Noise and weight precisions are also treated as random with gamma posterior distributions. Conceptually, for each input the network does not produce a single scalar, but an approximate predictive distribution: p(y | \\mathbf{x}, q) \\approx \\mathcal{N}(y | \\mu(\\mathbf{x}), \\sigma^2(\\mathbf{x})), where the distribution parameters are computed via propagation of means and variances through the network layers. Learning then proceeds in an assumed density filtering style. For each data point, we take the current approximate posterior, multiply it by the likelihood of that point, and project the result back to the Gaussian family: q_\\text{new}(\\boldsymbol{\\theta}) \\propto q_\\text{old}(\\theta)p(y | \\mathbf{x}, \\boldsymbol{\\theta}) \\quad \\text{(projected back to Gaussians)}. Usage Initialization First of all, you'll need to create a ProbabilisticBackpropagation instance. This is done slightly differently since at its core, the ProbabilisticBackpropagation has a bensemble.methods.probabilistic_backpropagation.PBPNet instance which is a network of probabilistic linear layers with ReLU activations. The simplest way to use the method is to specify layer sizes directly in the layer_sizes parameter: from bensemble.methods.probabilistic_backpropagation import ProbabilisticBackpropagation # Create ProbabilisticBackpropagation instance ensemble = ProbabilisticBackpropagation(layer_sizes=[1, 16, 16, 1]) Alternatively, you can create a PBPNet model and then specify it in the model parameter of the ProbabilisticBackpropagation constructor (see the API section for more info on PBPNet). You can also specify several optional parameters: noise_alpha: float = 6.0 - the alpha parameter of the Gamma distribution over the weight noise. noise_beta: float = 6.0 - the beta parameter of the Gamma distribution over the weight noise. weight_alpha: float = 6.0 - the alpha parameter of the Gamma distribution over the weight precision. weight_beta: float = 6.0 - the beta parameter of the Gamma distribution over the weight precision. dtype: torch.dtype = torch.float64 - value type for network. device: Optional[torch.device] = None - model device. Training For model training, it is sufficient to specify just a training data DataLoader : from torch.utils.data import DataLoader # Create train dataset train_data = ... # Create DataLoader train_loader = DataLoader(data, ...) # Train ensemble training_history = ensemble.fit(train_loader) You can also add several optional parameters, including: val_loader: Optional[torch.utils.data.DataLoader] = None - a DataLoader for model validation during training. num_epochs: int = 100 - the number of training epochs. step_clip: Optional[float] = 2.0 - value for step-wise clipping during training. The fit method returns a Dict[str, List[float]] containing training and validation RMSE and NLPD values acquired during model training: training_history.keys() > ['train_rmse', 'train_nlpd', 'val_rmse', 'val_nlpd'] Testing and model sampling When the ensemble is trained, you can make predictions and sample model just like with any other method in Bensemble: # Create test dataset test_data = ... # Make predictions for X in test_data: prediction, uncertainty = ensemble.predict(X, n_samples=50) print(prediction, uncertainty) # Sample models models = ensemble.sample_models(5) For more information on class methods and parameters, visit the API section .","title":"Bayes by Backprop"},{"location":"user-guide/bayes-by-backprop/#probabilistic-backpropagation","text":"Pro tip: check out our probabilistic backpropagation demo . We implement the probabilistic backpropagation (or Bayes by Backprop) method described by Jose Miguel Hernandez-Lobato and Ryan Adams (2015) in the bensemble.methods.probabilistic_backpropagation.ProbabilisticBackpropagation class (phew, that's a long one). This is a method that combines the backpropagation algorithm with Bayesian machine learning for efficient posterior optimization during model training. Note: currently the algorithm is implemented for regression with a Gaussian likelihood.","title":"Probabilistic Backpropagation"},{"location":"user-guide/bayes-by-backprop/#how-it-works","text":"Probabilistic Backpropagation (PBP) pushes the classical \u201cuncertainty in weights\u201d idea quite literally. PBP maintains a Gaussian distribution for model weights: w_{ij} = \\mathcal{N}(\\mu_{ij}, \\sigma^2_{ij}). Noise and weight precisions are also treated as random with gamma posterior distributions. Conceptually, for each input the network does not produce a single scalar, but an approximate predictive distribution: p(y | \\mathbf{x}, q) \\approx \\mathcal{N}(y | \\mu(\\mathbf{x}), \\sigma^2(\\mathbf{x})), where the distribution parameters are computed via propagation of means and variances through the network layers. Learning then proceeds in an assumed density filtering style. For each data point, we take the current approximate posterior, multiply it by the likelihood of that point, and project the result back to the Gaussian family: q_\\text{new}(\\boldsymbol{\\theta}) \\propto q_\\text{old}(\\theta)p(y | \\mathbf{x}, \\boldsymbol{\\theta}) \\quad \\text{(projected back to Gaussians)}.","title":"How it works"},{"location":"user-guide/bayes-by-backprop/#usage","text":"","title":"Usage"},{"location":"user-guide/bayes-by-backprop/#initialization","text":"First of all, you'll need to create a ProbabilisticBackpropagation instance. This is done slightly differently since at its core, the ProbabilisticBackpropagation has a bensemble.methods.probabilistic_backpropagation.PBPNet instance which is a network of probabilistic linear layers with ReLU activations. The simplest way to use the method is to specify layer sizes directly in the layer_sizes parameter: from bensemble.methods.probabilistic_backpropagation import ProbabilisticBackpropagation # Create ProbabilisticBackpropagation instance ensemble = ProbabilisticBackpropagation(layer_sizes=[1, 16, 16, 1]) Alternatively, you can create a PBPNet model and then specify it in the model parameter of the ProbabilisticBackpropagation constructor (see the API section for more info on PBPNet). You can also specify several optional parameters: noise_alpha: float = 6.0 - the alpha parameter of the Gamma distribution over the weight noise. noise_beta: float = 6.0 - the beta parameter of the Gamma distribution over the weight noise. weight_alpha: float = 6.0 - the alpha parameter of the Gamma distribution over the weight precision. weight_beta: float = 6.0 - the beta parameter of the Gamma distribution over the weight precision. dtype: torch.dtype = torch.float64 - value type for network. device: Optional[torch.device] = None - model device.","title":"Initialization"},{"location":"user-guide/bayes-by-backprop/#training","text":"For model training, it is sufficient to specify just a training data DataLoader : from torch.utils.data import DataLoader # Create train dataset train_data = ... # Create DataLoader train_loader = DataLoader(data, ...) # Train ensemble training_history = ensemble.fit(train_loader) You can also add several optional parameters, including: val_loader: Optional[torch.utils.data.DataLoader] = None - a DataLoader for model validation during training. num_epochs: int = 100 - the number of training epochs. step_clip: Optional[float] = 2.0 - value for step-wise clipping during training. The fit method returns a Dict[str, List[float]] containing training and validation RMSE and NLPD values acquired during model training: training_history.keys() > ['train_rmse', 'train_nlpd', 'val_rmse', 'val_nlpd']","title":"Training"},{"location":"user-guide/bayes-by-backprop/#testing-and-model-sampling","text":"When the ensemble is trained, you can make predictions and sample model just like with any other method in Bensemble: # Create test dataset test_data = ... # Make predictions for X in test_data: prediction, uncertainty = ensemble.predict(X, n_samples=50) print(prediction, uncertainty) # Sample models models = ensemble.sample_models(5) For more information on class methods and parameters, visit the API section .","title":"Testing and model sampling"},{"location":"user-guide/interface/","text":"Interface The interface of all of the models implemented in Bensemble is (almost) the same to provide interchangeability. In this section we will explore the common methods used in all of Bensemble's classes. Initialization ( __init__ ) All of the methods' constructors take different input arguments, however, one thing that they have in common is that they all require a nn.Module model as their first argument. This is because Bensemble models are in fact wrappers around PyTorch models allowing for posterior approximation rather than independent models themselves. fit All methods have a fit method that trains the model as well as calculates the parameters needed for model sampling and uncertainty estimation. The fit method always requires the train_loader parameter that is a torch.utils.data.DataLoader instance. You can additionally optionally provide a val_loader of the same type for validation. Keep in mind that if you use LaplaceApproximation , which is a post hoc method primarily used to approximate posterior weight distributions for pretrained models, the fit method will by default just call the compute_posterior method without model training (more on that in the section about Laplace approximation ). predict The predict method is use to make predictions for test objects with uncertainty estimation by sampling a given amount of model weights and making the corresponding amount of predctions. The parameters of predict in all models are: X: torch.Tensor : the object for which you are making a prediction. Keep in mind that it must be a torch.Tensor instance of the same shape as training objects. n_samples: int = 100 : the number of weight samples used to make the prediction. The default value of this parameter may vary across different methods depending on sampling speeds. The predict method outputs a tuple of torch.Tensor object containing the mean predictions and their uncertainties. sample_models This is the go-to method for model ensemble sampling. It takes the n_models parameter as input, defining the number of models to be sampled, and returns a list of sampled models as nn.Module instances. The sampled models are in fact just copies of the original model with perturbed weights. save This method takes a path file name as input and saves the ensemble state to the specified file. load This method takes a path file name as input and loads the ensemble state for the specified file.","title":"Interface"},{"location":"user-guide/interface/#interface","text":"The interface of all of the models implemented in Bensemble is (almost) the same to provide interchangeability. In this section we will explore the common methods used in all of Bensemble's classes.","title":"Interface"},{"location":"user-guide/interface/#initialization-__init__","text":"All of the methods' constructors take different input arguments, however, one thing that they have in common is that they all require a nn.Module model as their first argument. This is because Bensemble models are in fact wrappers around PyTorch models allowing for posterior approximation rather than independent models themselves.","title":"Initialization (__init__)"},{"location":"user-guide/interface/#fit","text":"All methods have a fit method that trains the model as well as calculates the parameters needed for model sampling and uncertainty estimation. The fit method always requires the train_loader parameter that is a torch.utils.data.DataLoader instance. You can additionally optionally provide a val_loader of the same type for validation. Keep in mind that if you use LaplaceApproximation , which is a post hoc method primarily used to approximate posterior weight distributions for pretrained models, the fit method will by default just call the compute_posterior method without model training (more on that in the section about Laplace approximation ).","title":"fit"},{"location":"user-guide/interface/#predict","text":"The predict method is use to make predictions for test objects with uncertainty estimation by sampling a given amount of model weights and making the corresponding amount of predctions. The parameters of predict in all models are: X: torch.Tensor : the object for which you are making a prediction. Keep in mind that it must be a torch.Tensor instance of the same shape as training objects. n_samples: int = 100 : the number of weight samples used to make the prediction. The default value of this parameter may vary across different methods depending on sampling speeds. The predict method outputs a tuple of torch.Tensor object containing the mean predictions and their uncertainties.","title":"predict"},{"location":"user-guide/interface/#sample_models","text":"This is the go-to method for model ensemble sampling. It takes the n_models parameter as input, defining the number of models to be sampled, and returns a list of sampled models as nn.Module instances. The sampled models are in fact just copies of the original model with perturbed weights.","title":"sample_models"},{"location":"user-guide/interface/#save","text":"This method takes a path file name as input and saves the ensemble state to the specified file.","title":"save"},{"location":"user-guide/interface/#load","text":"This method takes a path file name as input and loads the ensemble state for the specified file.","title":"load"},{"location":"user-guide/laplace-approximation/","text":"Kronecker-Factored Laplace Approximation Pro tip: check out our Kronecker-factored Laplace approximation demo . We implement the Kronecker-factored Laplace approximation method described by Hippolyt Ritter, Aleksandar Botev and David Barber (2018) in the bensemble.methods.laplace_approximation.LaplaceApproximation class. This is a post-hoc method for posterior approximation based on the Laplace approximation of the negative log-likelihood and Kronecker factorization of the Hessian in this approximation. How it works The Laplace approximation is based on second-order approximation of the negative log-posterior using Taylor decomposition around the MAP-estimate, which gives a Gaussian distribution centered a the MAP-estimate with the Hessian of the negative log-posterior as the covariance matrix: p(\\theta | \\mathcal{D}) \\approx \\mathcal{N}(\\theta; \\theta^* , \\bar{H}^{-1}). Since direct Hessian computation is still quite impractical, the Hessian block for each layer is further approximated as the Kronecker product of the covariance of inputs and the pre-activation Hessian: H_{\\lambda} \\approx \\mathbb{E}[\\mathcal{Q}_{\\lambda}] \\otimes \\mathbb{E}[\\mathcal{H}_{\\lambda}]. Then we can sample weights from the posterior as from the following matrix normal distribution: W_{\\lambda} \\sim \\mathcal{MN}(W_{\\lambda}^*, \\bar{\\mathcal{Q}}_{\\lambda}^{-1}, \\bar{\\mathcal{H}}_{\\lambda}^{-1}). The power of this method is in its post hoc nature: you can plug a pretrained model into it and quickly compute the posterior approximation without having to change the training procedure. Usage Initialization First of all, you'll need to create a LaplaceApproximation instance. This can be done as simple as: from bensemble.methods.laplace_approximation import LaplaceApproximation # Create your nn.Module model model = ... # Train your model ... # Create LaplaceApproximation instance ensemble = LaplaceApproximation(model) You can also specify several optional parameters: likelihood: str = \"classification\" - the likelihood to use. The current implementation supports cross entropy ( \"classification\" ) and MSE loss ( \"regression\" ). pretrained: bool = True - whether the model is pretrained. If not, .fit() will train the model before posterior computation (see below). verbose: bool = False - whether verbose for all methods is on. Verbose can be also toggled using the .toggle_verbose() method. Posterior computation and training Since Laplace approximation is post hoc, the default usage scenario is posterior computation over a pretrained model. You can do this by calling the compute_posterior method which takes a DataLoader as input. By default, the fit method does the same thing (it just calls compute_posterior ): from torch.utils.data import DataLoader # Create train dataset train_data = ... # Create DataLoader train_loader = DataLoader(data, ...) # Train ensemble ensemble.compute_posterior(train_loader) # The same as: ensemble.fit(train_loader) You can also add several optional parameters to compute_posterior , including: prior_precision: float = 0.0 - the precision of the prior distribution for reqularization. num_samples: int = 1000 - number of samples to use for posterior computation. Alternatively, you can pass pretrained=False to the ensemble constructor and then call the fit method to train the model and then compute the posterior: from bensemble.methods.laplace_approximation import LaplaceApproximation from torch.utils.data import DataLoader # Create your nn.Module model model = ... # Create untrained LaplaceApproximation instance ensemble = LaplaceApproximation(model, pretrained=False) # Create train dataset train_data = ... # Create DataLoader train_loader = DataLoader(data, ...) # Train ensemble ensemble.fit(train_loader) The optional parameters of fit are: num_epochs: int = 100 - the number of training epochs. lr: float = 1e-3 - the learning rate for model training. prior_precision: float = 1.0 - the precision of the prior distribution for reqularization. num_samples: int = 1000 - number of samples to use for posterior computation. The fit method returns a Dict[str, List[float]] containing training loss values acquired during model training: # Train loss train_loss = training_history[\"train_loss\"] Prediction and model sampling When the ensemble is trained, you can make predictions and sample model just like with any other method in Bensemble. Note that model sampling takes some time in this method, so a better approach to testing may be to sample an ensemble of models and then make all the predictions with them. # Create test dataset test_data = ... # Make predictions for X in test_data: prediction, uncertainty = ensemble.predict(X, n_samples=20) print(prediction, uncertainty) # Sample models models = ensemble.sample_models(10) The predict and sample_models also have a float temperature parameter which can be specified to scale the posterior distribution. For more information on class methods and parameters, visit the API section .","title":"Laplace Approximation"},{"location":"user-guide/laplace-approximation/#kronecker-factored-laplace-approximation","text":"Pro tip: check out our Kronecker-factored Laplace approximation demo . We implement the Kronecker-factored Laplace approximation method described by Hippolyt Ritter, Aleksandar Botev and David Barber (2018) in the bensemble.methods.laplace_approximation.LaplaceApproximation class. This is a post-hoc method for posterior approximation based on the Laplace approximation of the negative log-likelihood and Kronecker factorization of the Hessian in this approximation.","title":"Kronecker-Factored Laplace Approximation"},{"location":"user-guide/laplace-approximation/#how-it-works","text":"The Laplace approximation is based on second-order approximation of the negative log-posterior using Taylor decomposition around the MAP-estimate, which gives a Gaussian distribution centered a the MAP-estimate with the Hessian of the negative log-posterior as the covariance matrix: p(\\theta | \\mathcal{D}) \\approx \\mathcal{N}(\\theta; \\theta^* , \\bar{H}^{-1}). Since direct Hessian computation is still quite impractical, the Hessian block for each layer is further approximated as the Kronecker product of the covariance of inputs and the pre-activation Hessian: H_{\\lambda} \\approx \\mathbb{E}[\\mathcal{Q}_{\\lambda}] \\otimes \\mathbb{E}[\\mathcal{H}_{\\lambda}]. Then we can sample weights from the posterior as from the following matrix normal distribution: W_{\\lambda} \\sim \\mathcal{MN}(W_{\\lambda}^*, \\bar{\\mathcal{Q}}_{\\lambda}^{-1}, \\bar{\\mathcal{H}}_{\\lambda}^{-1}). The power of this method is in its post hoc nature: you can plug a pretrained model into it and quickly compute the posterior approximation without having to change the training procedure.","title":"How it works"},{"location":"user-guide/laplace-approximation/#usage","text":"","title":"Usage"},{"location":"user-guide/laplace-approximation/#initialization","text":"First of all, you'll need to create a LaplaceApproximation instance. This can be done as simple as: from bensemble.methods.laplace_approximation import LaplaceApproximation # Create your nn.Module model model = ... # Train your model ... # Create LaplaceApproximation instance ensemble = LaplaceApproximation(model) You can also specify several optional parameters: likelihood: str = \"classification\" - the likelihood to use. The current implementation supports cross entropy ( \"classification\" ) and MSE loss ( \"regression\" ). pretrained: bool = True - whether the model is pretrained. If not, .fit() will train the model before posterior computation (see below). verbose: bool = False - whether verbose for all methods is on. Verbose can be also toggled using the .toggle_verbose() method.","title":"Initialization"},{"location":"user-guide/laplace-approximation/#posterior-computation-and-training","text":"Since Laplace approximation is post hoc, the default usage scenario is posterior computation over a pretrained model. You can do this by calling the compute_posterior method which takes a DataLoader as input. By default, the fit method does the same thing (it just calls compute_posterior ): from torch.utils.data import DataLoader # Create train dataset train_data = ... # Create DataLoader train_loader = DataLoader(data, ...) # Train ensemble ensemble.compute_posterior(train_loader) # The same as: ensemble.fit(train_loader) You can also add several optional parameters to compute_posterior , including: prior_precision: float = 0.0 - the precision of the prior distribution for reqularization. num_samples: int = 1000 - number of samples to use for posterior computation. Alternatively, you can pass pretrained=False to the ensemble constructor and then call the fit method to train the model and then compute the posterior: from bensemble.methods.laplace_approximation import LaplaceApproximation from torch.utils.data import DataLoader # Create your nn.Module model model = ... # Create untrained LaplaceApproximation instance ensemble = LaplaceApproximation(model, pretrained=False) # Create train dataset train_data = ... # Create DataLoader train_loader = DataLoader(data, ...) # Train ensemble ensemble.fit(train_loader) The optional parameters of fit are: num_epochs: int = 100 - the number of training epochs. lr: float = 1e-3 - the learning rate for model training. prior_precision: float = 1.0 - the precision of the prior distribution for reqularization. num_samples: int = 1000 - number of samples to use for posterior computation. The fit method returns a Dict[str, List[float]] containing training loss values acquired during model training: # Train loss train_loss = training_history[\"train_loss\"]","title":"Posterior computation and training"},{"location":"user-guide/laplace-approximation/#prediction-and-model-sampling","text":"When the ensemble is trained, you can make predictions and sample model just like with any other method in Bensemble. Note that model sampling takes some time in this method, so a better approach to testing may be to sample an ensemble of models and then make all the predictions with them. # Create test dataset test_data = ... # Make predictions for X in test_data: prediction, uncertainty = ensemble.predict(X, n_samples=20) print(prediction, uncertainty) # Sample models models = ensemble.sample_models(10) The predict and sample_models also have a float temperature parameter which can be specified to scale the posterior distribution. For more information on class methods and parameters, visit the API section .","title":"Prediction and model sampling"},{"location":"user-guide/renyi-divergence/","text":"Renyi Divergence Variational Inference Pro tip: check out our Renyi divergence variational inference demo . We implement the Renyi divergence variational inference method described by Yingzhen Li and Richard E. Turner (2016) in the bensemble.methods.variational_renyi.VariationalRenyi class. This is a modification of the standard variational inference method that uses Renyi divergence instead of the usual KL-divergence for optimization. How it works Variational R\u00e9nyi inference keeps the spirit of standard variational inference method but replaces the usual KL-divergence by a whole family of divergences indexed by a parameter \u03b1. R\u00e9nyi's \u03b1-divergence for two distributions is defined as D_\\alpha(p\\|q) = \\frac{1}{\\alpha - 1}\\log\\int p(\\boldsymbol{\\theta})^\\alpha q(\\boldsymbol{\\theta})^{1-\\alpha}d\\boldsymbol{\\theta}. Qualitatively, this gives a knob that controls how aggressive or conservative the variational approximation is. Once trained, sampling networks is as simple as drawing from the Gaussian and plugging the sampled weights into the base model. Usage Initialization First of all, you'll need to create a VariationalRenyi instance. This can be done just by wrapping a model with it: from bensemble.methods.variational_renyi import VariationalRenyi # Create your nn.Module model model = ... # Create VariationalRenyi instance ensemble = VariationalRenyi(model) You can also specify several optional parameters: alpha: float = 1.0 - the \u03b1 parameter of Renyi divergence. prior_sigma: float = 1.0 - the variance of the prior distribution, which in this model is assumed to be a zero-mean Gaussian distribution. initial_rho: float = -2.0 - the initial rho parameter of the variational weight distribution. Training For model training, it is sufficient to specify just a training data DataLoader : from torch.utils.data import DataLoader # Create train dataset train_data = ... # Create DataLoader train_loader = DataLoader(data, ...) # Train ensemble training_history = ensemble.fit(train_loader) You can also add several optional parameters, including: val_loader: Optional[torch.utils.data.DataLoader] = None - a DataLoader for model validation during training. num_epochs: int = 100 - the number of training epochs. lr: float = 5e-4 - the learning rate for training. n_samples: int = 10 - the number of weights samples to use for predictions during validation. grad_clip: Optional[float] = 5.0 - gradient clipping parameter. The fit method returns a Dict[str, List[float]] containing training and validation loss values acquired during model training: # Train loss train_loss = training_history[\"train_loss\"] # Validation loss val_loss = training_history[\"val_loss\"] Testing and model sampling When the ensemble is trained, you can make predictions and sample model just like with any other method in Bensemble: # Create test dataset test_data = ... # Make predictions for X in test_data: prediction, uncertainty = ensemble.predict(X, n_samples=50) print(prediction, uncertainty) # Sample models models = ensemble.sample_models(5) For more information on class methods and parameters, visit the API section .","title":"Renyi Divergence"},{"location":"user-guide/renyi-divergence/#renyi-divergence-variational-inference","text":"Pro tip: check out our Renyi divergence variational inference demo . We implement the Renyi divergence variational inference method described by Yingzhen Li and Richard E. Turner (2016) in the bensemble.methods.variational_renyi.VariationalRenyi class. This is a modification of the standard variational inference method that uses Renyi divergence instead of the usual KL-divergence for optimization.","title":"Renyi Divergence Variational Inference"},{"location":"user-guide/renyi-divergence/#how-it-works","text":"Variational R\u00e9nyi inference keeps the spirit of standard variational inference method but replaces the usual KL-divergence by a whole family of divergences indexed by a parameter \u03b1. R\u00e9nyi's \u03b1-divergence for two distributions is defined as D_\\alpha(p\\|q) = \\frac{1}{\\alpha - 1}\\log\\int p(\\boldsymbol{\\theta})^\\alpha q(\\boldsymbol{\\theta})^{1-\\alpha}d\\boldsymbol{\\theta}. Qualitatively, this gives a knob that controls how aggressive or conservative the variational approximation is. Once trained, sampling networks is as simple as drawing from the Gaussian and plugging the sampled weights into the base model.","title":"How it works"},{"location":"user-guide/renyi-divergence/#usage","text":"","title":"Usage"},{"location":"user-guide/renyi-divergence/#initialization","text":"First of all, you'll need to create a VariationalRenyi instance. This can be done just by wrapping a model with it: from bensemble.methods.variational_renyi import VariationalRenyi # Create your nn.Module model model = ... # Create VariationalRenyi instance ensemble = VariationalRenyi(model) You can also specify several optional parameters: alpha: float = 1.0 - the \u03b1 parameter of Renyi divergence. prior_sigma: float = 1.0 - the variance of the prior distribution, which in this model is assumed to be a zero-mean Gaussian distribution. initial_rho: float = -2.0 - the initial rho parameter of the variational weight distribution.","title":"Initialization"},{"location":"user-guide/renyi-divergence/#training","text":"For model training, it is sufficient to specify just a training data DataLoader : from torch.utils.data import DataLoader # Create train dataset train_data = ... # Create DataLoader train_loader = DataLoader(data, ...) # Train ensemble training_history = ensemble.fit(train_loader) You can also add several optional parameters, including: val_loader: Optional[torch.utils.data.DataLoader] = None - a DataLoader for model validation during training. num_epochs: int = 100 - the number of training epochs. lr: float = 5e-4 - the learning rate for training. n_samples: int = 10 - the number of weights samples to use for predictions during validation. grad_clip: Optional[float] = 5.0 - gradient clipping parameter. The fit method returns a Dict[str, List[float]] containing training and validation loss values acquired during model training: # Train loss train_loss = training_history[\"train_loss\"] # Validation loss val_loss = training_history[\"val_loss\"]","title":"Training"},{"location":"user-guide/renyi-divergence/#testing-and-model-sampling","text":"When the ensemble is trained, you can make predictions and sample model just like with any other method in Bensemble: # Create test dataset test_data = ... # Make predictions for X in test_data: prediction, uncertainty = ensemble.predict(X, n_samples=50) print(prediction, uncertainty) # Sample models models = ensemble.sample_models(5) For more information on class methods and parameters, visit the API section .","title":"Testing and model sampling"},{"location":"user-guide/variational-inference/","text":"Practical Variational Inference Pro tip: check out our practical variational inference demo . We implement the practical variational inference method described by Graves (2011) in the bensemble.methods.variational_inference.VariationalEnsemble class. This is a simple method of in-training posterior approximation for linear neural networks that uses independent Gaussian distributions over the weights of the network. How it works The practical variational inference (PVI) methods follows the \"Bayesian layers\" approach: instead of treating the whole network as a single variational object, it replaces ordinary linear layers by Bayesian linear layers and adds a simple Gaussian likelihood on top. Each weight in a linear layer is assumed to have the following distribution: w_{ij} = \\mathcal{N}(\\mu_{ij}, \\sigma^2_{ij}) with a fixed Gaussian prior. We also implement the local reparametrization trick to allow for standard backpropagation through the Bayesian network and effeicient sampling: for an input mini-batch, the pre-activation means and variances are \\boldsymbol{\\gamma} = \\mathbf{xW}\\boldsymbol{\\mu}^\\top, \\quad \\boldsymbol{\\delta} = \\mathbf{xW}\\boldsymbol{\\sigma}^{2\\top} + \\text{bias term}, and activations are drawn as \\mathbf{z} = \\boldsymbol{\\gamma} + \\boldsymbol{\\varepsilon}\\odot\\boldsymbol{\\delta}, \\quad \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0, 1). The model is then optimized via a modified loss function that incorporates KL-divergence. Usage Initialization First of all, you'll need to create a VariationalEnsemble instance. This can be done as simple as: from bensemble.methods.variational_inference import VariationalEnsemble # Create your nn.Module model model = ... # Create VariationalEnsemble instance ensemble = VariationalEnsemble(model) You can also specify several optional parameters: likelihood: Optional[nn.Module] = None - custom likelihood function for the model. By default a Gaussian likelihood based on MSE for regression is used (see GaussianLikelihood in the API section for details). learning_rate: float = 1e-3 - learning rate for model training. prior_sigma: float = 1.0 - the variance of the prior distribution, which in this model is assumed to be a zero-mean Gaussian distribution. Training For model training, it is sufficient to specify just a training data DataLoader : from torch.utils.data import DataLoader # Create train dataset train_data = ... # Create DataLoader train_loader = DataLoader(data, ...) # Train ensemble ensemble.fit(train_loader) You can also add several optional parameters, including: epochs: int = 100 : the number of training epochs. kl_weight: float = 0.1 : the weight of the KL-divergence in the loss function. verbose: bool = True : option to turn training verbose on or off. Prediction and model sampling When the ensemble is trained, you can make predictions and sample model just like with any other method in Bensemble: # Create test dataset test_data = ... # Make predictions for X in test_data: prediction, uncertainty = ensemble.predict(X, n_samples=50) print(prediction, uncertainty) # Sample models models = ensemble.sample_models(5) For more information on class methods and parameters, visit the API section .","title":"Variational Inference"},{"location":"user-guide/variational-inference/#practical-variational-inference","text":"Pro tip: check out our practical variational inference demo . We implement the practical variational inference method described by Graves (2011) in the bensemble.methods.variational_inference.VariationalEnsemble class. This is a simple method of in-training posterior approximation for linear neural networks that uses independent Gaussian distributions over the weights of the network.","title":"Practical Variational Inference"},{"location":"user-guide/variational-inference/#how-it-works","text":"The practical variational inference (PVI) methods follows the \"Bayesian layers\" approach: instead of treating the whole network as a single variational object, it replaces ordinary linear layers by Bayesian linear layers and adds a simple Gaussian likelihood on top. Each weight in a linear layer is assumed to have the following distribution: w_{ij} = \\mathcal{N}(\\mu_{ij}, \\sigma^2_{ij}) with a fixed Gaussian prior. We also implement the local reparametrization trick to allow for standard backpropagation through the Bayesian network and effeicient sampling: for an input mini-batch, the pre-activation means and variances are \\boldsymbol{\\gamma} = \\mathbf{xW}\\boldsymbol{\\mu}^\\top, \\quad \\boldsymbol{\\delta} = \\mathbf{xW}\\boldsymbol{\\sigma}^{2\\top} + \\text{bias term}, and activations are drawn as \\mathbf{z} = \\boldsymbol{\\gamma} + \\boldsymbol{\\varepsilon}\\odot\\boldsymbol{\\delta}, \\quad \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(0, 1). The model is then optimized via a modified loss function that incorporates KL-divergence.","title":"How it works"},{"location":"user-guide/variational-inference/#usage","text":"","title":"Usage"},{"location":"user-guide/variational-inference/#initialization","text":"First of all, you'll need to create a VariationalEnsemble instance. This can be done as simple as: from bensemble.methods.variational_inference import VariationalEnsemble # Create your nn.Module model model = ... # Create VariationalEnsemble instance ensemble = VariationalEnsemble(model) You can also specify several optional parameters: likelihood: Optional[nn.Module] = None - custom likelihood function for the model. By default a Gaussian likelihood based on MSE for regression is used (see GaussianLikelihood in the API section for details). learning_rate: float = 1e-3 - learning rate for model training. prior_sigma: float = 1.0 - the variance of the prior distribution, which in this model is assumed to be a zero-mean Gaussian distribution.","title":"Initialization"},{"location":"user-guide/variational-inference/#training","text":"For model training, it is sufficient to specify just a training data DataLoader : from torch.utils.data import DataLoader # Create train dataset train_data = ... # Create DataLoader train_loader = DataLoader(data, ...) # Train ensemble ensemble.fit(train_loader) You can also add several optional parameters, including: epochs: int = 100 : the number of training epochs. kl_weight: float = 0.1 : the weight of the KL-divergence in the loss function. verbose: bool = True : option to turn training verbose on or off.","title":"Training"},{"location":"user-guide/variational-inference/#prediction-and-model-sampling","text":"When the ensemble is trained, you can make predictions and sample model just like with any other method in Bensemble: # Create test dataset test_data = ... # Make predictions for X in test_data: prediction, uncertainty = ensemble.predict(X, n_samples=50) print(prediction, uncertainty) # Sample models models = ensemble.sample_models(5) For more information on class methods and parameters, visit the API section .","title":"Prediction and model sampling"}]}